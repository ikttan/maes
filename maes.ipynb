{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M - Automated Essay Scoring\n",
    "_School of Information Technology_<br>\n",
    "_Monash University Malaysia_<br>\n",
    "(c) Copyright 2020, Ian Tan & Jun Qing Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "- Read dataset (ASAP)\n",
    "- Extract features (into file) using EASE\n",
    "- Conduct machine learning (Sci-kit Learn libraries)\n",
    "    - Naive Bayes\n",
    "    - SVR\n",
    "    - BLRR (later)\n",
    "- Evaluate (QWK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm #SVR is in SVM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the EASE functions, which is located in the ease folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'ease')\n",
    "import create\n",
    "import grade \n",
    "import model_creator \n",
    "import predictor_extractor \n",
    "import predictor_set \n",
    "import util_functions\n",
    "import essay_set\n",
    "import feature_extractor\n",
    "\n",
    "from essay_set import EssaySet\n",
    "from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AES (Hewlett Foundation dataset from Kaggle) in the folder `asap-aes`.  For this, we use the `training_set_rel3` for training and testing.  Note that the `test_set` and the `valid_set` cannot be used as they don't contain the scores and are meant for the competition to score the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"asap-aes/training_set_rel3.tsv\", sep='\\t', encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['essay'] = [entry.lower() for entry in data_set['essay']] # lower case for all words in essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 different essay sets.  As an overview:\n",
    "- Sets 1 & 2 are of persuasive/narrative in the form of letters\n",
    "- Sets 3, 4, 5 & 6 are source dependent response to a given essay\n",
    "- Sets 7 & 8 are of persuasive/narrative in the form of story writing essays\n",
    "\n",
    "These format makes it good for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_1 = data_set[data_set['essay_set'] == 1]\n",
    "data_set_2 = data_set[data_set['essay_set'] == 2]\n",
    "#data_set_3 = data_set[data_set['essay_set'] == 3]\n",
    "#data_set_4 = data_set[data_set['essay_set'] == 4]\n",
    "#data_set_5 = data_set[data_set['essay_set'] == 5]\n",
    "#data_set_6 = data_set[data_set['essay_set'] == 6]\n",
    "#data_set_7 = data_set[data_set['essay_set'] == 7]\n",
    "#data_set_8 = data_set[data_set['essay_set'] == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each set will retain the original index, we want each of them to have their own indexing so that it is easier to match the essay and the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_1 = data_set_1.reset_index() # resets index\n",
    "data_set_2 = data_set_2.reset_index()\n",
    "#data_set_3 = data_set_3.reset_index()\n",
    "#data_set_4 = data_set_4.reset_index()\n",
    "#data_set_5 = data_set_5.reset_index()\n",
    "#data_set_6 = data_set_6.reset_index()\n",
    "#data_set_7 = data_set_7.reset_index()\n",
    "#data_set_8 = data_set_8.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use just the `essay` content and the respective `scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want for the whole dataset.\n",
    "# Commented out as we will work on individual datasets\n",
    "#essays = data_set['essay']\n",
    "#scores = data_set['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_1 = data_set_1['essay']\n",
    "scores_1 = data_set_1['domain1_score']\n",
    "essays_2 = data_set_2['essay']\n",
    "scores_2 = data_set_2['domain1_score']\n",
    "#essays_3 = data_set_3['essay']\n",
    "#scores_3 = data_set_3['domain1_score']\n",
    "#essays_4 = data_set_4['essay']\n",
    "#scores_4 = data_set_4['domain1_score']\n",
    "#essays_5 = data_set_5['essay']\n",
    "#scores_5 = data_set_5['domain1_score']\n",
    "#essays_6 = data_set_6['essay']\n",
    "#scores_6 = data_set_6['domain1_score']\n",
    "#essays_7 = data_set_7['essay']\n",
    "#scores_7 = data_set_7['domain1_score']\n",
    "#essays_8 = data_set_8['essay']\n",
    "#scores_8 = data_set_8['domain1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the `domain1_score` column to `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1.columns = \"score\"\n",
    "scores_2.columns = \"score\"\n",
    "#scores_3.columns = \"score\"\n",
    "#scores_4.columns = \"score\"\n",
    "#scores_5.columns = \"score\"\n",
    "#scores_6.columns = \"score\"\n",
    "#scores_7.columns = \"score\"\n",
    "#scores_8.columns = \"score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE ABOVE NEEDS TO BE PUT INTO A LOOP BUT I LEFT IT AS IS BECAUSE YOU CAN PICK AND CHOOSE EASILY INSTEAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the essay sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these can be looped but I kept them separated for ease of readability and commenting out those that we don't need.  Each set takes a long time to process, and hence please be patient with this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_set_1 = EssaySet()\n",
    "e_set_2 = EssaySet()\n",
    "#e_set_3 = EssaySet()\n",
    "#e_set_4 = EssaySet()\n",
    "#e_set_5 = EssaySet()\n",
    "#e_set_6 = EssaySet()\n",
    "#e_set_7 = EssaySet()\n",
    "#e_set_8 = EssaySet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(essays_1)):\n",
    "    e_set_1.add_essay(essays_1[i], scores_1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(essays_2)):\n",
    "    e_set_2.add_essay(essays_2[i], scores_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left out for sets 3 - 6 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(essays_7)):\\n    e_set_7.add_essay(essays_7[i], scores_7[i])\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(len(essays_7)):\n",
    "    e_set_7.add_essay(essays_7[i], scores_7[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(essays_8)):\\n    e_set_8.add_essay(essays_8[i], scores_8[i])\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(len(essays_8)):\n",
    "    e_set_8.add_essay(essays_8[i], scores_8[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the next two variable assignment to change the evaluation of the essay sets.\n",
    "\n",
    "Would be better to do this above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_set = e_set_1\n",
    "score = scores_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = f_extractor.gen_length_feats(e_set)\n",
    "length_df = pd.DataFrame(\n",
    "    length, \n",
    "    columns = [\n",
    "        'chars', \n",
    "        'words', \n",
    "        'commas', \n",
    "        'apostrophes', \n",
    "        'punctuations', \n",
    "        'avg_word_length',\n",
    "        # new stuff, will need to compare original with new and separate punctuations\n",
    "        'sentences',\n",
    "        'questions',\n",
    "        'avg_word_sentence',\n",
    "        'POS', \n",
    "        'POS/total_words'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_*Exclude the prompts for the time being*_\n",
    "\n",
    "To be included next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this with the score based on the index\n",
    "# We use the shallow features first\n",
    "features = length_df\n",
    "dataset = features.merge(score, left_index=True, right_index=True)\n",
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "                   'avg_word_length', 'sentences', 'questions', 'avg_word_sentence',\n",
    "                   'POS', 'POS/total_words', 'score']\n",
    "#X_1 = dataset.iloc[:,0:10].values.astype(float)\n",
    "#y_1 = dataset.iloc[:,11].values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Essay Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_prompts = []\n",
    "\n",
    "#for i in range(1,8):\n",
    "# We use only for the first 2\n",
    "for i in range(1,2):\n",
    "    file = \"prompts/set\" + str(i) + \".txt\"\n",
    "    f = open(file, \"r\", encoding=\"latin-1\") # there are some 0x9x characters, hence need to specify encoding\n",
    "    essay_prompts.append(f.read())\n",
    "    \n",
    "def get_essay_prompt(essay_set):\n",
    "    return essay_prompts[essay_set-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<essay_set.EssaySet at 0x292dd11ac88>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsure how this works\n",
    "e_set.update_prompt(get_essay_prompt(1))\n",
    "\n",
    "# Need more explanation on how this works - look into EASE\n",
    "prompts = f_extractor.gen_prompt_feats(e_set)\n",
    "prompts_df = pd.DataFrame(prompts, columns = [\n",
    "    'prompt_words', 'prompt_words/total_words', 'synonym_words', 'synonym_words/total_words'\n",
    "])\n",
    "e_set # To check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another process that takes sometime to process\n",
    "unstemmed = util_functions.get_vocab_essays_count(e_set._text, e_set._score)\n",
    "stemmed = util_functions.get_vocab_essays_count(e_set._clean_stem_text, e_set._score)\n",
    "\n",
    "bow = list(map(lambda a,b:[a,b], unstemmed, stemmed))\n",
    "bow_df = pd.DataFrame(bow, columns = ['unstemmed', 'stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.904884</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>12.397403</td>\n",
       "      <td>0.031870</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.398458</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.226221</td>\n",
       "      <td>446</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2309.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.030501</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>21.761905</td>\n",
       "      <td>0.047412</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.339869</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.183007</td>\n",
       "      <td>548</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.032258</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.142857</td>\n",
       "      <td>22.814815</td>\n",
       "      <td>0.073596</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.487097</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>391</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.452962</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.916667</td>\n",
       "      <td>20.070175</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.315331</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.203833</td>\n",
       "      <td>678</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2615.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.028846</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>15.058140</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.405769</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.205769</td>\n",
       "      <td>587</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  1908.0  389.0    18.0          7.0           5.0         4.904884   \n",
       "1  2309.0  459.0    12.0          5.0           1.0         5.030501   \n",
       "2  1560.0  310.0     9.0          6.0           0.0         5.032258   \n",
       "3  3130.0  574.0    13.0          7.0           2.0         5.452962   \n",
       "4  2615.0  520.0    13.0          7.0           0.0         5.028846   \n",
       "\n",
       "   sentences  questions  avg_word_sentence        POS  POS/total_words  \\\n",
       "0       10.0        2.0          38.900000  12.397403         0.031870   \n",
       "1       18.0        1.0          25.500000  21.761905         0.047412   \n",
       "2       14.0        0.0          22.142857  22.814815         0.073596   \n",
       "3       24.0        1.0          23.916667  20.070175         0.034965   \n",
       "4       30.0        0.0          17.333333  15.058140         0.028958   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0         155.0                  0.398458           88.0   \n",
       "1         156.0                  0.339869           84.0   \n",
       "2         151.0                  0.487097          100.0   \n",
       "3         181.0                  0.315331          117.0   \n",
       "4         211.0                  0.405769          107.0   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  \n",
       "0                   0.226221        446      434  \n",
       "1                   0.183007        548      524  \n",
       "2                   0.322581        391      369  \n",
       "3                   0.203833        678      650  \n",
       "4                   0.205769        587      565  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.concat([length_df, prompts_df, bow_df], axis=1, sort=False)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.904884</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>12.397403</td>\n",
       "      <td>0.031870</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.398458</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.226221</td>\n",
       "      <td>446</td>\n",
       "      <td>434</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2309.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.030501</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>21.761905</td>\n",
       "      <td>0.047412</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.339869</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.183007</td>\n",
       "      <td>548</td>\n",
       "      <td>524</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.032258</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.142857</td>\n",
       "      <td>22.814815</td>\n",
       "      <td>0.073596</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.487097</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>391</td>\n",
       "      <td>369</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.452962</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.916667</td>\n",
       "      <td>20.070175</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.315331</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.203833</td>\n",
       "      <td>678</td>\n",
       "      <td>650</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2615.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.028846</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>15.058140</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.405769</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.205769</td>\n",
       "      <td>587</td>\n",
       "      <td>565</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  1908.0  389.0    18.0          7.0           5.0         4.904884   \n",
       "1  2309.0  459.0    12.0          5.0           1.0         5.030501   \n",
       "2  1560.0  310.0     9.0          6.0           0.0         5.032258   \n",
       "3  3130.0  574.0    13.0          7.0           2.0         5.452962   \n",
       "4  2615.0  520.0    13.0          7.0           0.0         5.028846   \n",
       "\n",
       "   sentences  questions  avg_word_sentence        POS  POS/total_words  \\\n",
       "0       10.0        2.0          38.900000  12.397403         0.031870   \n",
       "1       18.0        1.0          25.500000  21.761905         0.047412   \n",
       "2       14.0        0.0          22.142857  22.814815         0.073596   \n",
       "3       24.0        1.0          23.916667  20.070175         0.034965   \n",
       "4       30.0        0.0          17.333333  15.058140         0.028958   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0         155.0                  0.398458           88.0   \n",
       "1         156.0                  0.339869           84.0   \n",
       "2         151.0                  0.487097          100.0   \n",
       "3         181.0                  0.315331          117.0   \n",
       "4         211.0                  0.405769          107.0   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  domain1_score  \n",
       "0                   0.226221        446      434              8  \n",
       "1                   0.183007        548      524              9  \n",
       "2                   0.322581        391      369              7  \n",
       "3                   0.203833        678      650             10  \n",
       "4                   0.205769        587      565              8  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export features to a file for next stage (optional)\n",
    "dataset = features.merge(score, left_index=True, right_index=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "                   'avg_word_length', 'sentences', 'questions', 'avg_word_sentence',\n",
    "                   'POS', 'POS/total_words',\n",
    "                   'score']\n",
    "\"\"\"\n",
    "\n",
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "                   'avg_word_length', 'sentences', 'questions', 'avg_word_sentence',\n",
    "                   'POS', 'POS/total_words',\n",
    "                   'prompt_words', 'prompt_words/total_words', 'synonym_words',\n",
    "                   'synonym_words/total_words', 'unstemmed', 'stemmed',\n",
    "                   'score']\n",
    "dataset.head()\n",
    "dataset.to_csv('maes_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can just use the features and score for the X and y but just to keep to certain convention if reading back from the CSV file above.\n",
    "\n",
    "**YOU CAN RUN FROM HERE ON BY READING THE FEATURES FOR THE TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('maes_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.,  9.,  7., 10.,  8.,  8., 10., 10.,  9.,  9.,  8.,  8.,  7.,\n",
       "        6.,  6., 12.,  8.,  8.,  4.,  6.,  8.,  3., 10., 11.,  8.,  9.,\n",
       "        4.,  9.,  9.,  8., 10., 10.,  6.,  8.,  9., 10., 12.,  8., 10.,\n",
       "        7.,  2.,  8.,  6.,  8.,  8.,  8.,  8., 11.,  6.,  5.,  9.,  7.,\n",
       "        8., 10.,  8., 10.,  9.,  7.,  8.,  4.,  8.,  8.,  8.,  7.,  9.,\n",
       "        9.,  8.,  9.,  7., 12., 10., 10.,  8.,  7.,  8.,  8., 10., 10.,\n",
       "       10.,  8.,  8.,  8.,  7.,  6., 10.,  8., 10.,  9.,  6.,  7.,  8.,\n",
       "       11., 11.,  8., 10.,  7.,  8., 11.,  8.,  7., 10.,  8.,  9., 10.,\n",
       "        9., 11., 10.,  8.,  8.,  6., 11.,  9.,  8.,  8.,  9.,  4.,  8.,\n",
       "       12., 10.,  8.,  8.,  9., 10.,  7.,  5.,  8.,  9.,  9., 10., 10.,\n",
       "       10., 11., 10., 10.,  7.,  9.,  7., 10.,  7.,  9., 10., 10.,  7.,\n",
       "        9., 10.,  6., 12.,  9.,  8.,  8.,  8.,  6.,  9., 12., 10.,  8.,\n",
       "        9.,  9., 10.,  8., 10., 12.,  8.,  8.,  9., 10.,  8.,  9.,  6.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8., 10.,  8.,  8.,  8., 10., 10.,  8.,\n",
       "        8.,  8.,  8.,  8.,  5.,  9.,  6., 10.,  8., 11.,  8., 10.,  5.,\n",
       "        8., 10., 10., 11.,  8.,  9.,  8., 10., 10., 10., 10.,  8.,  6.,\n",
       "        6.,  9.,  6.,  7.,  8.,  8., 10.,  8., 10.,  8.,  9., 10.,  6.,\n",
       "        9.,  8.,  8., 12.,  8.,  9.,  8.,  8.,  9.,  8., 12.,  7.,  8.,\n",
       "        7.,  6.,  8.,  9.,  9.,  8.,  9.,  9., 11.,  7.,  7., 11.,  8.,\n",
       "        9.,  9., 10.,  9.,  9.,  6., 10.,  8.,  8.,  8., 11.,  9.,  7.,\n",
       "        9.,  8., 10.,  8., 11., 11., 10.,  8.,  9.,  8.,  8., 10.,  8.,\n",
       "        9., 11.,  8.,  9.,  8., 10., 10.,  7.,  8.,  6.,  8., 11.,  4.,\n",
       "       12.,  8., 10., 11., 11., 11.,  8., 10., 10., 10.,  9.,  8.,  8.,\n",
       "        8.,  9., 10., 10.,  9., 10.,  8.,  9.,  6.,  9., 10.,  9.,  7.,\n",
       "        9., 10.,  9., 10.,  8.,  9.,  8., 12.,  8., 10.,  8.,  9., 10.,\n",
       "        6.,  8.,  8., 11., 10.,  8.,  9., 10.,  9., 11., 10.,  8., 11.,\n",
       "        9.,  8.,  9.,  9., 10., 12.,  7.,  8.,  8.,  8.,  9.,  7.,  5.,\n",
       "        8.,  6.,  8.,  8.,  7.,  2.,  6., 10., 10.,  8., 11.,  9.,  8.,\n",
       "        8.,  9.,  8., 10.,  8., 10.,  8.,  7.,  4.,  8., 10., 11.,  9.,\n",
       "       10., 10.,  7., 11.,  8., 11., 10.,  8.,  7.,  8., 10., 12.,  8.,\n",
       "        8.,  7.,  6.,  7., 10., 10.,  8.,  8.,  6.,  8.,  8.,  8.,  9.,\n",
       "        8.,  8.,  8.,  8.,  8., 12., 10.,  9.,  8.,  7.,  6.,  8., 12.,\n",
       "        6.,  8.,  6.,  7.,  9.,  9., 10.,  9., 11.,  8.,  9.,  8., 11.,\n",
       "        8., 11.,  7., 11.,  8.,  8.,  8., 10.,  8.,  8., 10.,  8.,  8.,\n",
       "        8., 10., 11.,  9.,  2.,  7.,  8.,  7.,  9.,  8.,  4.,  7., 10.,\n",
       "        9.,  8.,  8.,  8.,  8.,  8.,  7.,  8.,  9.,  8.,  8.,  9.,  8.,\n",
       "        8., 12.,  7.,  6.,  9.,  8.,  6.,  8., 10.,  8., 10.,  9.,  9.,\n",
       "        7., 10.,  9.,  8.,  8.,  9.,  8.,  8.,  7.,  8.,  9.,  8.,  9.,\n",
       "        9., 12.,  9., 10.,  8.,  7.,  8., 10., 10.,  7., 10.,  9., 11.,\n",
       "        7.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8., 12.,  6.,  7.,  9.,\n",
       "       10.,  8.,  9.,  7.,  8.,  6.,  7., 10., 10.,  8.,  8., 11.,  8.,\n",
       "        8.,  8.,  8.,  9.,  8.,  8., 10.,  8.,  8.,  2.,  6.,  8.,  8.,\n",
       "        8., 10.,  8.,  6.,  8.,  8., 11., 10., 10., 10., 12.,  8.,  8.,\n",
       "       10.,  7.,  8.,  9.,  8.,  8.,  8.,  8.,  8.,  9., 11., 10.,  8.,\n",
       "       10.,  8.,  6.,  9.,  8.,  8.,  8., 11.,  9.,  9., 11.,  8.,  9.,\n",
       "       10.,  5.,  6.,  8.,  8., 10.,  7.,  8.,  6.,  8.,  8.,  8.,  6.,\n",
       "        8.,  8., 11.,  9.,  6.,  8.,  9.,  7.,  7.,  7.,  9., 10.,  8.,\n",
       "        6.,  9.,  8.,  9.,  8., 11.,  7.,  8.,  8.,  6.,  8., 11.,  7.,\n",
       "        8., 11., 11.,  8.,  6., 10.,  8.,  5., 10.,  9.,  8.,  8.,  6.,\n",
       "        7.,  7.,  8., 10.,  8.,  8.,  9., 11.,  6.,  8., 10.,  8.,  9.,\n",
       "        8., 10.,  7.,  9.,  9.,  8., 10., 11.,  8.,  6.,  8.,  9.,  7.,\n",
       "        8.,  9., 10.,  8.,  8.,  8.,  9.,  6.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  9.,  8., 11., 10.,  8.,  8.,  9.,  8.,  9.,  8.,  9.,  6.,\n",
       "        6.,  8.,  8.,  9.,  8.,  8.,  6., 12.,  8.,  8.,  6.,  8.,  7.,\n",
       "        8., 10.,  8.,  9., 10., 11., 10.,  9., 10.,  9.,  8.,  9.,  8.,\n",
       "       10.,  7., 10.,  9., 10., 10.,  8.,  6., 10.,  6.,  8.,  7., 10.,\n",
       "        8.,  9.,  8., 10.,  9., 10.,  8.,  6.,  9.,  8.,  8., 10., 10.,\n",
       "        9.,  9.,  8.,  8.,  8., 10.,  8.,  8.,  7.,  8.,  9.,  9.,  8.,\n",
       "        9.,  8., 10.,  9.,  8., 10.,  8.,  9.,  7.,  8.,  8.,  8., 10.,\n",
       "        8.,  9., 10.,  8.,  8.,  7.,  7., 10.,  9.,  8.,  9., 10., 11.,\n",
       "        7., 10.,  8.,  8.,  8.,  8.,  7.,  9.,  8.,  8., 11.,  8.,  8.,\n",
       "        8., 10.,  9.,  8., 10.,  9.,  8.,  6.,  8.,  9.,  6., 11.,  7.,\n",
       "        9.,  8.,  8.,  8.,  8.,  8., 10.,  9., 12.,  8.,  8.,  5.,  9.,\n",
       "        8.,  8.,  9.,  6., 10.,  6.,  9.,  8., 10.,  8.,  2.,  9., 10.,\n",
       "       10.,  8.,  8.,  5., 10.,  8.,  7.,  8.,  8.,  8., 10.,  8., 12.,\n",
       "        6.,  7., 10.,  9., 10.,  9.,  8.,  8., 10.,  8.,  9., 12.,  8.,\n",
       "        8.,  8., 10., 10.,  8.,  7.,  8., 12.,  6.,  9., 10.,  8.,  9.,\n",
       "       12., 11.,  8., 11.,  9.,  8., 10.,  8.,  9.,  9.,  8.,  7., 11.,\n",
       "        9., 10.,  8.,  8.,  9., 10.,  4., 10., 10.,  8.,  9.,  8.,  8.,\n",
       "       10.,  8., 10.,  8.,  8.,  6., 11.,  9., 10., 10.,  8.,  8., 10.,\n",
       "       10.,  9., 10., 11.,  6., 10., 11.,  8.,  8.,  8.,  6.,  8.,  7.,\n",
       "        7.,  7., 10., 10.,  8.,  8., 12.,  8.,  8.,  9.,  7., 11., 11.,\n",
       "        6.,  8., 12.,  9.,  9.,  8.,  8.,  8., 10.,  9.,  8., 10.,  7.,\n",
       "        8., 12., 10.,  8.,  9.,  9.,  8.,  9.,  7.,  8.,  9.,  8., 11.,\n",
       "        7.,  7., 11.,  9.,  8., 11.,  9.,  8.,  6.,  2.,  8.,  8.,  7.,\n",
       "        8., 10., 10.,  8.,  8.,  8.,  8.,  9., 11.,  8.,  8.,  9.,  8.,\n",
       "        6.,  5.,  9.,  9.,  4.,  7., 10.,  9., 12., 12.,  8.,  8.,  9.,\n",
       "        6.,  9.,  8., 10., 10.,  8.,  8.,  8.,  9.,  7.,  8., 10.,  8.,\n",
       "        8., 10., 10.,  8.,  9., 11.,  8.,  9.,  8., 10.,  8., 10., 10.,\n",
       "        8.,  9.,  8., 10.,  8.,  8.,  9.,  8., 11.,  8.,  6.,  6.,  8.,\n",
       "        9.,  7.,  9.,  9., 11., 10., 10., 11.,  7.,  9.,  9.,  8.,  8.,\n",
       "        6., 10.,  8.,  8.,  8.,  8., 10., 10.,  9.,  7.,  9.,  9.,  9.,\n",
       "       10.,  7.,  8.,  8.,  8.,  8.,  8., 10.,  8., 12.,  8.,  8.,  7.,\n",
       "        9.,  7.,  9., 10.,  8., 11.,  8.,  8.,  6., 10., 10.,  8.,  8.,\n",
       "        8.,  4., 10.,  9.,  8.,  9.,  8.,  8.,  6., 10.,  9.,  9.,  4.,\n",
       "        9.,  8.,  8., 11.,  8., 10., 10.,  9.,  8.,  7.,  8.,  9.,  8.,\n",
       "        2., 10.,  6.,  9.,  8., 10.,  8.,  8.,  8.,  6.,  8., 12.,  6.,\n",
       "       11.,  8.,  8.,  8., 10.,  9.,  6.,  8.,  8., 11.,  9.,  8.,  9.,\n",
       "        9., 10.,  8., 10.,  8.,  9.,  8.,  8., 10.,  6.,  9., 12.,  8.,\n",
       "        8.,  8., 10.,  9.,  8.,  9., 10.,  8., 10., 12., 12.,  7.,  8.,\n",
       "       10.,  9.,  8.,  6.,  9.,  8.,  8., 10.,  7.,  8., 10.,  8.,  9.,\n",
       "       10.,  8.,  9.,  9.,  8.,  8., 10., 10., 11.,  8.,  8.,  7.,  8.,\n",
       "        9., 12.,  8.,  8.,  9.,  8., 11.,  8.,  7.,  8.,  8.,  9.,  9.,\n",
       "        8., 10.,  8.,  9.,  4.,  9.,  8., 10., 10.,  6.,  9.,  9.,  8.,\n",
       "        8.,  8.,  8.,  8., 10.,  6., 10.,  9.,  8., 10., 11., 10., 11.,\n",
       "       10.,  9.,  8.,  9.,  8., 10.,  7.,  8.,  8., 10., 12.,  8., 11.,\n",
       "        4.,  8., 11.,  8.,  9., 12., 10.,  8.,  9.,  8.,  9.,  9., 10.,\n",
       "        8.,  8.,  8., 11., 10.,  8.,  9., 10.,  6.,  7.,  8.,  7.,  8.,\n",
       "        9.,  6.,  8., 10., 10.,  8.,  8.,  8., 10., 10.,  7.,  8.,  8.,\n",
       "        8.,  7.,  9.,  4.,  9.,  8.,  7., 11.,  7.,  8., 10.,  7., 11.,\n",
       "       11.,  8.,  9.,  9.,  8.,  7., 10.,  9.,  8., 10.,  9.,  9.,  8.,\n",
       "        8.,  9., 11.,  9.,  9.,  8.,  8., 10.,  8., 11.,  8.,  7.,  8.,\n",
       "        9.,  9.,  8.,  8.,  7., 10.,  8.,  9.,  8.,  8., 10.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  7., 11., 10.,  9.,  8., 10.,  8.,  8.,  8.,\n",
       "        8., 11., 11.,  8., 10.,  9.,  8.,  9.,  8.,  8.,  6.,  7.,  9.,\n",
       "        9., 12.,  8.,  8., 10., 10.,  8.,  8.,  9., 10., 12.,  8., 10.,\n",
       "        9., 10.,  9.,  9.,  7.,  8.,  8.,  8.,  8.,  5., 11.,  6.,  9.,\n",
       "        9.,  7.,  9.,  8.,  9., 10.,  9., 11., 10.,  9.,  9., 10.,  8.,\n",
       "        9.,  7., 11., 10.,  9.,  8., 12.,  8., 10.,  8., 10.,  8.,  6.,\n",
       "        8.,  8., 10.,  8., 10., 11.,  6.,  7.,  8.,  6.,  8.,  8.,  9.,\n",
       "       10.,  8.,  8.,  9., 10., 10.,  8.,  6., 12., 10.,  8., 10., 12.,\n",
       "        9.,  9., 10.,  9., 12.,  9., 11.,  8., 10.,  7.,  9.,  8., 10.,\n",
       "        6.,  7.,  8., 10.,  8.,  8., 10.,  9.,  8., 10.,  8.,  8.,  6.,\n",
       "        4.,  8.,  9.,  6.,  9.,  7.,  9.,  7.,  7.,  6.,  9.,  9.,  8.,\n",
       "        8.,  8.,  9.,  6., 11.,  8., 11., 10.,  8.,  6.,  9.,  8., 10.,\n",
       "        8.,  8., 10.,  9.,  8.,  9.,  8.,  9.,  8.,  8.,  8.,  7.,  8.,\n",
       "        5.,  8., 10.,  8.,  8., 10.,  8.,  9.,  8., 10.,  8.,  6.,  8.,\n",
       "       10., 10.,  6.,  8.,  8.,  8.,  6.,  6.,  9.,  6.,  4.,  8.,  9.,\n",
       "        9.,  8.,  8., 10.,  8., 10.,  8., 10.,  7.,  8.,  6.,  8.,  9.,\n",
       "        6.,  7.,  8.,  2.,  8.,  8.,  8.,  7.,  9.,  9.,  4.,  6.,  8.,\n",
       "        8.,  8., 11.,  7.,  5., 11.,  7.,  8., 10.,  8.,  9.,  8.,  8.,\n",
       "        9.,  8.,  8., 11., 10.,  8.,  9.,  9.,  7., 10., 10., 10., 11.,\n",
       "        9.,  2., 10.,  8.,  8.,  9., 11., 10.,  9.,  8.,  8., 10.,  8.,\n",
       "        8.,  8.,  9.,  9.,  8.,  9., 11.,  8.,  8.,  9.,  9.,  9.,  9.,\n",
       "        8.,  8.,  9.,  7., 10., 10.,  9.,  8.,  8.,  8.,  8., 10., 10.,\n",
       "       10.,  8.,  8.,  8.,  8., 10., 10.,  8.,  8., 10.,  8.,  8.,  8.,\n",
       "       11.,  8., 11.,  8.,  6., 11.,  9.,  8.,  9.,  8.,  8.,  9.,  8.,\n",
       "        8.,  8.,  9.,  9.,  8., 10.,  8.,  9.,  8.,  9.,  9., 10.,  9.,\n",
       "       10.,  9., 11.,  6.,  5.,  8., 11.,  8., 10.,  8.,  8.,  8.,  6.,\n",
       "        8.,  7.,  8.,  8.,  5., 10.,  8.,  7.,  9., 11.,  9., 10., 10.,\n",
       "        9.,  9.,  8.,  8.,  8.,  9.,  8.,  8., 11.,  8., 12.,  6.,  7.,\n",
       "        7.,  8.,  9.,  6.,  9.,  8.,  6., 10.,  7.,  8.,  9.,  8., 11.,\n",
       "        9.,  9.,  9., 10.,  9.,  9., 10.,  9.,  8., 10.,  9.,  8.,  8.,\n",
       "        9.,  7., 11.,  8.,  7., 10.,  8.,  8.,  6.,  9., 11., 10., 10.,\n",
       "        8.,  9.,  6.,  8.,  8.,  9.,  8., 10.,  8., 10.,  8., 10., 10.,\n",
       "       12.,  8.,  8., 12.,  8.,  5.,  8.,  9.,  8.,  9., 10.,  8., 10.,\n",
       "        8., 10.,  9.,  9., 11.,  5., 10.,  9., 10.,  8.,  8.,  7.,  8.,\n",
       "        2.,  7.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:,1:16].values.astype(float)\n",
    "y = dataset.iloc[:,18].values.astype(float)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 15)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "source": [
    "### Split the train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train, X_1_test, y_1_train, y_1_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Have a look at the first few lines\n",
    "print(y_1_test[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scaling used for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_trainNB = X_1_train\n",
    "y_1_trainNB = y_1_train\n",
    "X_1_testNB = X_1_test\n",
    "y_1_testNB = y_1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb_1 = naive_bayes.MultinomialNB()\n",
    "model_nb_1.fit(X_1_trainNB, y_1_trainNB.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the Naive Bayes model is called `model_nb_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use standard scaler for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_Xsvm = StandardScaler()\n",
    "sc_ysvm = StandardScaler()\n",
    "X_1_trainSVM = sc_Xsvm.fit_transform(X_1_train)\n",
    "y_1_trainSVM = sc_ysvm.fit_transform(y_1_train)\n",
    "X_1_testSVM = sc_Xsvm.transform(X_1_test)\n",
    "y_1_testSVM = sc_ysvm.transform(y_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "# maybe use poly and increase the degree\n",
    "model_svm_1 = SVR(kernel='rbf', gamma='auto', verbose=True)\n",
    "#regressor = SVR(kernel='poly', degree=5, gamma='auto', verbose=True)\n",
    "model_svm_1.fit(X_1_trainSVM,y_1_trainSVM.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the Support Vector Machine (SVM) model is called `model_svm_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_Xblrr = StandardScaler()\n",
    "sc_yblrr = StandardScaler()\n",
    "X_1_trainBLRR = sc_Xblrr.fit_transform(X_1_train)\n",
    "y_1_trainBLRR = sc_yblrr.fit_transform(y_1_train)\n",
    "X_1_testBLRR = sc_Xblrr.transform(X_1_test)\n",
    "y_1_testBLRR = sc_yblrr.transform(y_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model_blrr_1 = linear_model.BayesianRidge()\n",
    "model_blrr_1.fit(X_1_trainBLRR, y_1_trainBLRR.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the Bayesian Linear Ridge Regression (BLRR) model is called `model_blrr_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the respective validation set and will have to also pre-process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  3  2  7  4  4  0  0  0  0]\n",
      " [ 0  0  0  7  4 12  0  1  0  0]\n",
      " [ 0  0  1  4 11 95 18 15  0  0]\n",
      " [ 0  0  0  0  2 19 13 19  1  1]\n",
      " [ 0  0  0  1  1  5 21 36  8  6]\n",
      " [ 0  0  0  0  0  0  4  7  2  7]\n",
      " [ 0  0  0  0  0  0  0  2  1  5]]\n"
     ]
    }
   ],
   "source": [
    "y_1_predNB = model_nb_1.predict(X_1_testNB)\n",
    "#y_1_predNB = sc_ynb.inverse_transform(y_1_predNB).round()\n",
    "\n",
    "cm = confusion_matrix(y_1_test, y_1_predNB)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  3  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0  0  0]\n",
      " [ 0  0  1  8  6  5  0  0  0  0]\n",
      " [ 0  0  0  0 14  8  2  0  0  0]\n",
      " [ 0  0  0  3 16 79 40  6  0  0]\n",
      " [ 0  0  0  0  1 13 29 11  1  0]\n",
      " [ 0  0  0  0  0  3 26 42  7  0]\n",
      " [ 0  0  0  0  0  0  4 11  5  0]\n",
      " [ 0  0  0  0  0  0  0  3  5  0]]\n"
     ]
    }
   ],
   "source": [
    "y_1_predSVM = model_svm_1.predict(X_1_testSVM)\n",
    "y_1_predSVM = sc_ysvm.inverse_transform(y_1_predSVM).round()\n",
    "\n",
    "cm = confusion_matrix(y_1_test, y_1_predSVM)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_1_predSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  3  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  7 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 14  7  2  0  0  0  0]\n",
      " [ 0  0  0  3 25 73 38  5  0  0  0]\n",
      " [ 0  0  0  0  3 10 32  9  0  1  0]\n",
      " [ 0  0  0  0  0  3 28 35 12  0  0]\n",
      " [ 0  0  0  0  0  1  2  9  6  2  0]\n",
      " [ 0  0  0  0  0  0  0  2  4  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "y_1_predBLRR = model_blrr_1.predict(X_1_testBLRR)\n",
    "y_1_predBLRR = sc_yblrr.inverse_transform(y_1_predBLRR).round()\n",
    "\n",
    "cm = confusion_matrix(y_1_test, y_1_predBLRR)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using QWK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWK scores for NB, SVR and BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.80      1.00      0.89         4\n",
      "         4.0       0.40      1.00      0.57         2\n",
      "         5.0       0.00      0.00      0.00         2\n",
      "         6.0       0.35      0.35      0.35        20\n",
      "         7.0       0.18      0.17      0.17        24\n",
      "         8.0       0.70      0.66      0.68       144\n",
      "         9.0       0.23      0.24      0.23        55\n",
      "        10.0       0.45      0.46      0.46        78\n",
      "        11.0       0.17      0.10      0.12        20\n",
      "        12.0       0.26      0.62      0.37         8\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       357\n",
      "   macro avg       0.35      0.46      0.39       357\n",
      "weighted avg       0.48      0.47      0.47       357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rpt = classification_report(y_1_test, y_1_predNB)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7914276903145422\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_1_test, y_1_predNB, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.00      0.00      0.00         4\n",
      "         4.0       0.50      0.50      0.50         2\n",
      "         5.0       0.20      0.50      0.29         2\n",
      "         6.0       0.67      0.40      0.50        20\n",
      "         7.0       0.38      0.58      0.46        24\n",
      "         8.0       0.72      0.55      0.62       144\n",
      "         9.0       0.29      0.53      0.37        55\n",
      "        10.0       0.58      0.54      0.56        78\n",
      "        11.0       0.28      0.25      0.26        20\n",
      "        12.0       0.00      0.00      0.00         8\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       357\n",
      "   macro avg       0.36      0.38      0.36       357\n",
      "weighted avg       0.54      0.50      0.51       357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rpt = classification_report(y_1_test, y_1_predSVM)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7981076983547645\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_1_test, y_1_predSVM, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.00      0.00      0.00         4\n",
      "         4.0       0.00      0.00      0.00         2\n",
      "         5.0       0.00      0.00      0.00         2\n",
      "         6.0       0.44      0.35      0.39        20\n",
      "         7.0       0.25      0.58      0.35        24\n",
      "         8.0       0.78      0.51      0.61       144\n",
      "         9.0       0.31      0.58      0.41        55\n",
      "        10.0       0.58      0.45      0.51        78\n",
      "        11.0       0.27      0.30      0.29        20\n",
      "        12.0       0.25      0.12      0.17         8\n",
      "        13.0       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       357\n",
      "   macro avg       0.26      0.26      0.25       357\n",
      "weighted avg       0.55      0.47      0.49       357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rpt = classification_report(y_1_test, y_1_predBLRR)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026556723826195\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_1_test, y_1_predBLRR, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to execute the notebook is 1375.2518405914307\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Total time to execute the notebook is \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWK scores output are from -1 to 1, where -1 means that it is totally wrong while 1 is a perfect match (classification).  The aim is to get as close as possible to 1, with a score of 0.6 being generally accepted as a good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the output of the QWK agreements, the score is just \"moderate agreement\".  Work now is to achieve substantial agreement.\n",
    "\n",
    "https://www.statisticshowto.com/cohens-kappa-statistic/\n",
    "\n",
    "In short, SVM works a little better than Naive Bayes for AES and similarly with BLRR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QWK Scores (Manual Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 7.07333635e-06, 2.82933454e-05, ...,\n",
       "        9.89389995e-01, 9.94687924e-01, 1.00000000e+00],\n",
       "       [7.07333635e-06, 0.00000000e+00, 7.07333635e-06, ...,\n",
       "        9.84106213e-01, 9.89389995e-01, 9.94687924e-01],\n",
       "       [2.82933454e-05, 7.07333635e-06, 0.00000000e+00, ...,\n",
       "        9.78836578e-01, 9.84106213e-01, 9.89389995e-01],\n",
       "       ...,\n",
       "       [9.89389995e-01, 9.84106213e-01, 9.78836578e-01, ...,\n",
       "        0.00000000e+00, 7.07333635e-06, 2.82933454e-05],\n",
       "       [9.94687924e-01, 9.89389995e-01, 9.84106213e-01, ...,\n",
       "        7.07333635e-06, 0.00000000e+00, 7.07333635e-06],\n",
       "       [1.00000000e+00, 9.94687924e-01, 9.89389995e-01, ...,\n",
       "        2.82933454e-05, 7.07333635e-06, 0.00000000e+00]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(cm) # Just to get the same size as the confusion matrix from above\n",
    "w = np.zeros((N,N)) # create a matrix of N by N\n",
    "d = (N-1)**2 # the weighted portion\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(w)):\n",
    "        w[i][j] = float(((i-j)**2)/d) \n",
    "w # The weighted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-fe5d55babad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test_test' is not defined"
     ]
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-1194af1797d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predNB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_predNB' is not defined"
     ]
    }
   ],
   "source": [
    "np.unique(y_predNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-84892cbf4f08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mact_hist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_test_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mact_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test_test' is not defined"
     ]
    }
   ],
   "source": [
    "act_hist=np.zeros([N])\n",
    "for item in y_test_test: \n",
    "    act_hist[item-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hist=np.zeros([N])\n",
    "for item in y_predNB: \n",
    "    pred_hist[item-1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.outer(act_hist, pred_hist)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = E/E.sum()\n",
    "E.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm/cm.sum()\n",
    "cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "den=0\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(w)):\n",
    "        num+=w[i][j]*cm[i][j]\n",
    "        den+=w[i][j]*E[i][j]\n",
    "            \n",
    "weighted_kappa = (1 - (num/den))\n",
    "weighted_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
