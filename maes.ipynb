{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M - Automated Essay Scoring\n",
    "_School of Information Technology_<br>\n",
    "_Monash University Malaysia_<br>\n",
    "(c) Copyright 2020, Ian Tan & Jun Qing Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "- Read dataset (ASAP)\n",
    "- Extract features (into file) using EASE\n",
    "- Conduct machine learning (Sci-kit Learn libraries)\n",
    "    - Naive Bayes\n",
    "    - SVR\n",
    "    - BLRR (later)\n",
    "- Evaluate (QWK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm #SVR is in SVM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the EASE functions, which is located in the ease folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'ease')\n",
    "import create\n",
    "import grade \n",
    "import model_creator \n",
    "import predictor_extractor \n",
    "import predictor_set \n",
    "import util_functions\n",
    "import essay_set\n",
    "import feature_extractor\n",
    "\n",
    "from essay_set import EssaySet\n",
    "from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AES (Hewlett Foundation dataset from Kaggle) in the folder `asap-aes`.  For this, we use the `training_set_rel3` for training and testing.  Note that the `test_set` and the `valid_set` cannot be used as they don't contain the scores and are meant for the competition to score the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"asap-aes/training_set_rel3.tsv\", sep='\\t', encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['essay'] = [entry.lower() for entry in data_set['essay']] # lower case for all words in essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 different essay sets.  As an overview:\n",
    "- Sets 1 & 2 are of persuasive/narrative in the form of letters\n",
    "- Sets 3, 4, 5 & 6 are source dependent response to a given essay\n",
    "- Sets 7 & 8 are of persuasive/narrative in the form of story writing essays\n",
    "\n",
    "These format makes it good for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_1 = data_set[data_set['essay_set'] == 1]\n",
    "data_set_2 = data_set[data_set['essay_set'] == 2]\n",
    "#data_set_3 = data_set[data_set['essay_set'] == 3]\n",
    "#data_set_4 = data_set[data_set['essay_set'] == 4]\n",
    "#data_set_5 = data_set[data_set['essay_set'] == 5]\n",
    "#data_set_6 = data_set[data_set['essay_set'] == 6]\n",
    "data_set_7 = data_set[data_set['essay_set'] == 7]\n",
    "data_set_8 = data_set[data_set['essay_set'] == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each set will retain the original index, we want each of them to have their own indexing so that it is easier to match the essay and the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_1 = data_set_1.reset_index() # resets index\n",
    "data_set_2 = data_set_2.reset_index()\n",
    "#data_set_3 = data_set_3.reset_index()\n",
    "#data_set_4 = data_set_4.reset_index()\n",
    "#data_set_5 = data_set_5.reset_index()\n",
    "#data_set_6 = data_set_6.reset_index()\n",
    "data_set_7 = data_set_7.reset_index()\n",
    "data_set_8 = data_set_8.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use just the `essay` content and the respective `scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want for the whole dataset.\n",
    "# Commented out as we will work on individual datasets\n",
    "#essays = data_set['essay']\n",
    "#scores = data_set['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_1 = data_set_1['essay']\n",
    "scores_1 = data_set_1['domain1_score']\n",
    "essays_2 = data_set_2['essay']\n",
    "scores_2 = data_set_2['domain1_score']\n",
    "#essays_3 = data_set_3['essay']\n",
    "#scores_3 = data_set_3['domain1_score']\n",
    "#essays_4 = data_set_4['essay']\n",
    "#scores_4 = data_set_4['domain1_score']\n",
    "#essays_5 = data_set_5['essay']\n",
    "#scores_5 = data_set_5['domain1_score']\n",
    "#essays_6 = data_set_6['essay']\n",
    "#scores_6 = data_set_6['domain1_score']\n",
    "essays_7 = data_set_7['essay']\n",
    "scores_7 = data_set_7['domain1_score']\n",
    "essays_8 = data_set_8['essay']\n",
    "scores_8 = data_set_8['domain1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the `domain1_score` column to `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1.columns = \"score\"\n",
    "scores_2.columns = \"score\"\n",
    "#scores_3.columns = \"score\"\n",
    "#scores_4.columns = \"score\"\n",
    "#scores_5.columns = \"score\"\n",
    "#scores_6.columns = \"score\"\n",
    "scores_7.columns = \"score\"\n",
    "scores_8.columns = \"score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE ABOVE NEEDS TO BE PUT INTO A LOOP BUT I LEFT IT AS IS BECAUSE YOU CAN PICK AND CHOOSE EASILY INSTEAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the essay sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these can be looped but I kept them separated for ease of readability and commenting out those that we don't need.  Each set takes a long time to process, and hence please be patient with this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_set_1 = EssaySet()\n",
    "e_set_2 = EssaySet()\n",
    "#e_set_3 = EssaySet()\n",
    "#e_set_4 = EssaySet()\n",
    "#e_set_5 = EssaySet()\n",
    "#e_set_6 = EssaySet()\n",
    "e_set_7 = EssaySet()\n",
    "e_set_8 = EssaySet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(essays_1)):\n",
    "    e_set_1.add_essay(essays_1[i], scores_1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(essays_2)):\n",
    "    e_set_2.add_essay(essays_2[i], scores_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left out for sets 3 - 6 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(essays_7)):\n",
    "    e_set_7.add_essay(essays_7[i], scores_7[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(essays_8)):\n",
    "    e_set_8.add_essay(essays_8[i], scores_8[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently only doing for Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the next two variable assignment to change the evaluation of the essay sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_set = e_set_7\n",
    "score = scores_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = f_extractor.gen_length_feats(e_set)\n",
    "length_df_1 = pd.DataFrame(\n",
    "    length, \n",
    "    columns = [\n",
    "        'chars', \n",
    "        'words', \n",
    "        'commas', \n",
    "        'apostrophes', \n",
    "        'punctuations', \n",
    "        'avg_word_length',\n",
    "        # new stuff\n",
    "        'sentences',\n",
    "        'questions',\n",
    "        'avg_word_sentence',\n",
    "        'POS', \n",
    "        'POS/total_words'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_*Exclude the prompts for the time being*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this with the score based on the index\n",
    "# We use the shallow features first\n",
    "features = length_df_1\n",
    "dataset = features.merge(score, left_index=True, right_index=True)\n",
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "                   'avg_word_length', 'sentences', 'questions', 'avg_word_sentence',\n",
    "                   'POS', 'POS/total_words', 'score']\n",
    "X_1 = dataset.iloc[:,0:10].values.astype(float)\n",
    "y_1 = dataset.iloc[:,11].values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1569, 10)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1569, 1)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = np.array(y_1).reshape(-1,1)\n",
    "y_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.]\n",
      " [24.]\n",
      " [13.]\n",
      " [17.]\n",
      " [ 8.]]\n"
     ]
    }
   ],
   "source": [
    "### Split the train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train, X_1_test, y_1_train, y_1_test = train_test_split(X_1, y_1, test_size=0.2, random_state=0)\n",
    "# Have a look at the first few lines\n",
    "print(y_1_test[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scaling used for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_trainNB = X_1_train\n",
    "y_1_trainNB = y_1_train\n",
    "X_1_testNB = X_1_test\n",
    "y_1_testNB = y_1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb_1 = naive_bayes.MultinomialNB()\n",
    "model_nb_1.fit(X_1_trainNB, y_1_trainNB.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the Naive Bayes model is called `model_nb_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use standard scaler for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_Xsvm = StandardScaler()\n",
    "sc_ysvm = StandardScaler()\n",
    "X_1_trainSVM = sc_Xsvm.fit_transform(X_1_train)\n",
    "y_1_trainSVM = sc_ysvm.fit_transform(y_1_train)\n",
    "X_1_testSVM = sc_Xsvm.transform(X_1_test)\n",
    "y_1_testSVM = sc_ysvm.transform(y_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "# maybe use poly and increase the degree\n",
    "model_svm_1 = SVR(kernel='rbf', gamma='auto', verbose=True)\n",
    "#regressor = SVR(kernel='poly', degree=5, gamma='auto', verbose=True)\n",
    "model_svm_1.fit(X_1_trainSVM,y_1_trainSVM.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the Support Vector Machine (SVM) model is called `model_svm_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_Xblrr = StandardScaler()\n",
    "sc_yblrr = StandardScaler()\n",
    "X_1_trainBLRR = sc_Xblrr.fit_transform(X_1_train)\n",
    "y_1_trainBLRR = sc_yblrr.fit_transform(y_1_train)\n",
    "X_1_testBLRR = sc_Xblrr.transform(X_1_test)\n",
    "y_1_testBLRR = sc_yblrr.transform(y_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model_blrr_1 = linear_model.BayesianRidge()\n",
    "model_blrr_1.fit(X_1_trainBLRR, y_1_trainBLRR.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the Bayesian Linear Ridge Regression (BLRR) model is called `model_blrr_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the respective validation set and will have to also pre-process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  3  1  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  3  4  3  1  0  0  2  0  0  3  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  2  2  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  1  0  1  0  0  0  1  5  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  1  1  2  1  0  0  1  0  2  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  3  1  3  2  0  0  0  0  0  1  0  0  0  0  0  0  0  1]\n",
      " [ 1  0  0  0  1  2  5  3  0  0  0  0  1  6  2  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  2  2  0  0  3  0  0  1  7  2  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  1  2  1  1  0  2  2  0 12  2  1  0  0  1  0  1  1]\n",
      " [ 0  0  0  0  0  2  5  1  1  0  1  0  0  8  2  1  1  1  5  3  0  2]\n",
      " [ 0  0  0  1  0  1  2  0  0  0  1  1  2 10  6  0  0  4  6  1  1  2]\n",
      " [ 0  0  0  0  0  0  1  0  1  0  0  1  0  5  7  0  0  1  7  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  3  5  2  0  0  0  1  2  1]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  1  0  1  4  2  0  0  0  2  3  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  1  6  2  1  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  4  1  1  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  0  0  0  1  1  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  4  2  3  8]]\n"
     ]
    }
   ],
   "source": [
    "y_1_predNB = model_nb_1.predict(X_1_testNB)\n",
    "#y_1_predNB = sc_ynb.inverse_transform(y_1_predNB).round()\n",
    "\n",
    "cm = confusion_matrix(y_1_test, y_1_predNB)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  2  0  0  0  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  6  5  2  3  0  1  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  2  1  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  2  1  1  3  0  1  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  1  0  2  2  1  0  0  1  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  3  3  0  1  1  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  1  1  2  2  3  3  4  2  2  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  1  0  1  4  4  4  1  2  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  1  0  3  8  4  1  3  4  2  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  5  3  4  4  3  3  6  3  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  7  6 10  5  6  3  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  1  1  3  5  6  5  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  1  8  1  0  1  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  1  3  5  1  2  0  2  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  3  7  2  1  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  4  2  2  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  2  1  1  0  1  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  6  5  4  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "y_1_predSVM = model_svm_1.predict(X_1_testSVM)\n",
    "y_1_predSVM = sc_ysvm.inverse_transform(y_1_predSVM).round()\n",
    "\n",
    "cm = confusion_matrix(y_1_test, y_1_predSVM)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_1_predSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  2  2  0  1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  8  5  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  2  0  1  4  3  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  3  2  2  1  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  2  5  2  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  4  6  4  4  1  1  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  4  7  1  3  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  5  9  2  5  3  1  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  4  8  4  4  5  1  1  2  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  8  3 11  6  5  1  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  1  3  5  6  3  4  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  7  1  0  1  1  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  1  1  6  2  1  2  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  2  6  1  3  1  1  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  1  2  2  1  1  2  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  1  0  2  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  7  1  4  4  2  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "y_1_predBLRR = model_blrr_1.predict(X_1_testBLRR)\n",
    "y_1_predBLRR = sc_yblrr.inverse_transform(y_1_predBLRR).round()\n",
    "\n",
    "cm = confusion_matrix(y_1_test, y_1_predBLRR)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using QWK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWK scores for NB, SVR and BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.00      0.00      0.00         0\n",
      "         4.0       0.00      0.00      0.00         1\n",
      "         5.0       0.00      0.00      0.00         0\n",
      "         6.0       0.00      0.00      0.00         2\n",
      "         7.0       0.00      0.00      0.00         7\n",
      "         8.0       0.18      0.22      0.20        18\n",
      "         9.0       0.07      0.33      0.11         6\n",
      "        10.0       0.00      0.00      0.00        11\n",
      "        11.0       0.20      0.10      0.13        10\n",
      "        12.0       0.00      0.00      0.00        11\n",
      "        13.0       0.00      0.00      0.00        22\n",
      "        14.0       0.00      0.00      0.00        18\n",
      "        15.0       0.00      0.00      0.00        28\n",
      "        16.0       0.11      0.24      0.16        33\n",
      "        17.0       0.19      0.16      0.17        38\n",
      "        18.0       0.00      0.00      0.00        24\n",
      "        19.0       0.00      0.00      0.00        14\n",
      "        20.0       0.00      0.00      0.00        16\n",
      "        21.0       0.17      0.35      0.23        17\n",
      "        22.0       0.07      0.09      0.08        11\n",
      "        23.0       0.00      0.00      0.00         7\n",
      "        24.0       0.28      0.40      0.33        20\n",
      "\n",
      "   micro avg       0.11      0.11      0.11       314\n",
      "   macro avg       0.06      0.09      0.06       314\n",
      "weighted avg       0.08      0.11      0.09       314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rpt = classification_report(y_1_test, y_1_predNB)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6474149199029923\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_1_test, y_1_predNB, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         4.0       0.00      0.00      0.00         1\n",
      "         6.0       0.00      0.00      0.00         2\n",
      "         7.0       0.00      0.00      0.00         7\n",
      "         8.0       0.00      0.00      0.00        18\n",
      "         9.0       0.00      0.00      0.00         6\n",
      "        10.0       0.07      0.09      0.08        11\n",
      "        11.0       0.00      0.00      0.00        10\n",
      "        12.0       0.20      0.27      0.23        11\n",
      "        13.0       0.12      0.14      0.13        22\n",
      "        14.0       0.17      0.22      0.19        18\n",
      "        15.0       0.12      0.14      0.13        28\n",
      "        16.0       0.17      0.12      0.14        33\n",
      "        17.0       0.23      0.26      0.25        38\n",
      "        18.0       0.19      0.25      0.22        24\n",
      "        19.0       0.00      0.00      0.00        14\n",
      "        20.0       0.00      0.00      0.00        16\n",
      "        21.0       0.11      0.06      0.08        17\n",
      "        22.0       0.00      0.00      0.00        11\n",
      "        23.0       0.00      0.00      0.00         7\n",
      "        24.0       0.00      0.00      0.00        20\n",
      "\n",
      "   micro avg       0.11      0.11      0.11       314\n",
      "   macro avg       0.07      0.08      0.07       314\n",
      "weighted avg       0.11      0.11      0.11       314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rpt = classification_report(y_1_test, y_1_predSVM)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7200545006594828\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_1_test, y_1_predSVM, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         4.0       0.00      0.00      0.00         1\n",
      "         6.0       0.00      0.00      0.00         2\n",
      "         7.0       0.00      0.00      0.00         7\n",
      "         8.0       0.00      0.00      0.00        18\n",
      "         9.0       0.00      0.00      0.00         6\n",
      "        10.0       0.00      0.00      0.00        11\n",
      "        11.0       0.08      0.10      0.09        10\n",
      "        12.0       0.08      0.18      0.11        11\n",
      "        13.0       0.16      0.27      0.20        22\n",
      "        14.0       0.14      0.39      0.21        18\n",
      "        15.0       0.07      0.07      0.07        28\n",
      "        16.0       0.09      0.12      0.11        33\n",
      "        17.0       0.18      0.16      0.17        38\n",
      "        18.0       0.14      0.12      0.13        24\n",
      "        19.0       0.05      0.07      0.06        14\n",
      "        20.0       0.00      0.00      0.00        16\n",
      "        21.0       0.12      0.06      0.08        17\n",
      "        22.0       0.17      0.18      0.17        11\n",
      "        23.0       0.00      0.00      0.00         7\n",
      "        24.0       0.25      0.05      0.08        20\n",
      "        25.0       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.11      0.11      0.11       314\n",
      "   macro avg       0.07      0.08      0.07       314\n",
      "weighted avg       0.10      0.11      0.10       314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rpt = classification_report(y_1_test, y_1_predBLRR)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684625181033411\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_1_test, y_1_predBLRR, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to execute the notebook is 304.00941824913025\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Total time to execute the notebook is \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collate the essay prompts\n",
    "This consist of one essay from each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_prompts = []\n",
    "\n",
    "# Takes a bit of time also :)\n",
    "for i in range(1,9):\n",
    "    file = \"prompts/set\" + str(i) + \".txt\"\n",
    "    f = open(file, \"r\", encoding=\"latin-1\") # there are some 0x9x characters, hence need to specify encoding\n",
    "    essay_prompts.append(f.read())\n",
    "    \n",
    "def get_essay_prompt(essay_set):\n",
    "    return essay_prompts[essay_set-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-845889c2657d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Unsure how this works\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0me_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_prompt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_essay_prompt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Need more explanation on how this works - look into EASE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e_set' is not defined"
     ]
    }
   ],
   "source": [
    "# Unsure how this works\n",
    "e_set.update_prompt(get_essay_prompt(2))\n",
    "\n",
    "# Need more explanation on how this works - look into EASE\n",
    "\n",
    "prompts = f_extractor.gen_prompt_feats(e_set)\n",
    "prompts_df = pd.DataFrame(prompts, columns = ['prompt_words', 'prompt_words/total_words', 'synonym_words', 'synonym_words/total_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another process that takes sometime to process\n",
    "unstemmed = util_functions.get_vocab_essays_count(e_set._text, e_set._score)\n",
    "stemmed = util_functions.get_vocab_essays_count(e_set._clean_stem_text, e_set._score)\n",
    "\n",
    "bow = list(map(lambda a,b:[a,b], unstemmed, stemmed))\n",
    "bow_df = pd.DataFrame(bow, columns = ['unstemmed', 'stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([length_df, prompts_df, bow_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export features to a file for next stage (optional)\n",
    "dataset = features.merge(scores, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "       'avg_word_length', 'POS', 'POS/total_words', 'prompt_words',\n",
    "       'prompt_words/total_words', 'synonym_words',\n",
    "       'synonym_words/total_words', 'unstemmed', 'stemmed', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('maes_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can just use the features and score for the X and y but just to keep to certain convention if reading back from the CSV file above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:13].values.astype(float)\n",
    "y = dataset.iloc[:,14].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the train / test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Have a look at the first few lines\n",
    "print(y_test[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "# maybe use poly and increase the degree\n",
    "regressor = SVR(kernel='rbf', gamma='auto', verbose=True)\n",
    "#regressor = SVR(kernel='poly', degree=5, gamma='auto', verbose=True)\n",
    "regressor.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test / Predict the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used yet as I don't have a sample X\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = sc_y.inverse_transform(y_pred).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Real Values':sc_y.inverse_transform(y_test.reshape(-1)),\n",
    "        'Predicted Values':y_pred\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = sc_y.inverse_transform(y_test).round()\n",
    "# y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to wrap my head around this (where's the predictor)\n",
    "\n",
    "print(\"accuracy score:\", regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy score:\", accuracy_score(df['Real Values'], df['Predicted Values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohen_kappa_score(sc_y.inverse_transform(y_test).round(), y_pred, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test = sc_X.inverse_transform(X_train)\n",
    "X_train_test = X_train_test.astype(int)\n",
    "X_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_test = sc_y.inverse_transform(y_train.reshape(-1))\n",
    "y_train_test = y_train_test.astype(int)\n",
    "y_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbclassifier = naive_bayes.MultinomialNB()\n",
    "nbclassifier.fit(X_train_test, y_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_test = sc_X.inverse_transform(X_test)\n",
    "X_test_test = X_test_test.astype(int)\n",
    "X_test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_test = sc_y.inverse_transform(y_test.reshape(-1))\n",
    "y_test_test = y_test_test.astype(int)\n",
    "y_test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predNB = nbclassifier.predict(X_test_test)\n",
    "\n",
    "cm = confusion_matrix(y_test_test, y_predNB)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rpt = classification_report(y_test_test, y_predNB)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QWK Scores (Manual Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(cm) # Just to get the same size as the confusion matrix from above\n",
    "w = np.zeros((N,N)) # create a matrix of N by N\n",
    "d = (N-1)**2 # the weighted portion\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(w)):\n",
    "        w[i][j] = float(((i-j)**2)/d) \n",
    "w # The weighted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_predNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_hist=np.zeros([N])\n",
    "for item in y_test_test: \n",
    "    act_hist[item-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hist=np.zeros([N])\n",
    "for item in y_predNB: \n",
    "    pred_hist[item-1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.outer(act_hist, pred_hist)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = E/E.sum()\n",
    "E.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm/cm.sum()\n",
    "cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "den=0\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(w)):\n",
    "        num+=w[i][j]*cm[i][j]\n",
    "        den+=w[i][j]*E[i][j]\n",
    "            \n",
    "weighted_kappa = (1 - (num/den))\n",
    "weighted_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWK scores output are from -1 to 1, where -1 means that it is totally wrong while 1 is a perfect match (classification).  The aim is to get as close as possible to 1, with a score of 0.6 being generally accepted as a good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QWK for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is a manual computation of the QWK, which we later found that it is already available as an option with the [Cohen Kappa Score](https://journals.sagepub.com/doi/10.1177/001316446002000104) in sklearn, when we set the weights to 'quadratic'.  Since it has already been manually coded above, we use the sklearn.metrics.cohen_kappa_score to validate our manual coded scoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohen_kappa_score(y_test_test, y_predNB))\n",
    "print(cohen_kappa_score(y_test_test, y_predNB, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the output of the QWK agreements, the score is just \"moderate agreement\".  Work now is to achieve substantial agreement.\n",
    "\n",
    "https://www.statisticshowto.com/cohens-kappa-statistic/\n",
    "\n",
    "In short, SVM works a little better than Naive Bayes for AES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
