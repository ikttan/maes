{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M - Automated Essay Scoring\n",
    "_School of Information Technology_<br>\n",
    "_Monash University Malaysia_<br>\n",
    "(c) Copyright 2020, Ian Tan & Jun Qing Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "- Read dataset (ASAP)\n",
    "- Extract features (into file) using EASE\n",
    "- Conduct machine learning (Sci-kit Learn libraries)\n",
    "    - Naive Bayes\n",
    "    - SVR\n",
    "    - BLRR\n",
    "- Evaluate (QWK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm #SVR is in SVM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the EASE functions, which is located in the ease folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'ease')\n",
    "import create\n",
    "import grade \n",
    "import model_creator \n",
    "import predictor_extractor \n",
    "import predictor_set \n",
    "import util_functions\n",
    "import essay_set\n",
    "import feature_extractor\n",
    "\n",
    "from essay_set import EssaySet\n",
    "from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AES (Hewlett Foundation dataset from Kaggle) in the folder `asap-aes`.  For this, we use the `training_set_rel3` for training and the `valid_set` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"asap-aes/training_set_rel3.tsv\", sep='\\t', encoding=\"latin-1\")\n",
    "test_set = pd.read_csv(\"asap-aes/test_set.tsv\", sep='\\t', encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['essay'] = [entry.lower() for entry in train_set['essay']] # lower case for all words in essay\n",
    "test_set['essay'] = [entry.lower() for entry in test_set['essay']] # lower case for all words in essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 different essay sets.  As an overview:\n",
    "- Sets 1 & 2 are of persuasive/narrative in the form of letters\n",
    "- Sets 3, 4, 5 & 6 are source dependent response to a given essay\n",
    "- Sets 7 & 8 are of persuasive/narrative in the form of story writing essays\n",
    "\n",
    "These format makes it good for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_1 = train_set[train_set['essay_set'] == 1]\n",
    "train_set_2 = train_set[train_set['essay_set'] == 2]\n",
    "train_set_3 = train_set[train_set['essay_set'] == 3]\n",
    "train_set_4 = train_set[train_set['essay_set'] == 4]\n",
    "train_set_5 = train_set[train_set['essay_set'] == 5]\n",
    "train_set_6 = train_set[train_set['essay_set'] == 6]\n",
    "train_set_7 = train_set[train_set['essay_set'] == 7]\n",
    "train_set_8 = train_set[train_set['essay_set'] == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do similarly for the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_predictionid</th>\n",
       "      <th>domain2_predictionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2383</td>\n",
       "      <td>1</td>\n",
       "      <td>i believe that computers have a positive effec...</td>\n",
       "      <td>2383</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2384</td>\n",
       "      <td>1</td>\n",
       "      <td>dear @caps1, i know some problems have came up...</td>\n",
       "      <td>2384</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2385</td>\n",
       "      <td>1</td>\n",
       "      <td>dear to whom it @month1 concern, computers are...</td>\n",
       "      <td>2385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2386</td>\n",
       "      <td>1</td>\n",
       "      <td>dear @caps1 @caps2, @caps3 has come to my atte...</td>\n",
       "      <td>2386</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2387</td>\n",
       "      <td>1</td>\n",
       "      <td>dear local newspaper, i think that people have...</td>\n",
       "      <td>2387</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0      2383          1  i believe that computers have a positive effec...   \n",
       "1      2384          1  dear @caps1, i know some problems have came up...   \n",
       "2      2385          1  dear to whom it @month1 concern, computers are...   \n",
       "3      2386          1  dear @caps1 @caps2, @caps3 has come to my atte...   \n",
       "4      2387          1  dear local newspaper, i think that people have...   \n",
       "\n",
       "   domain1_predictionid  domain2_predictionid  \n",
       "0                  2383                   NaN  \n",
       "1                  2384                   NaN  \n",
       "2                  2385                   NaN  \n",
       "3                  2386                   NaN  \n",
       "4                  2387                   NaN  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1 = test_set[test_set['essay_set'] == 1]\n",
    "test_set_2 = test_set[test_set['essay_set'] == 2]\n",
    "test_set_3 = test_set[test_set['essay_set'] == 3]\n",
    "test_set_4 = test_set[test_set['essay_set'] == 4]\n",
    "test_set_5 = test_set[test_set['essay_set'] == 5]\n",
    "test_set_6 = test_set[test_set['essay_set'] == 6]\n",
    "test_set_7 = test_set[test_set['essay_set'] == 7]\n",
    "test_set_8 = test_set[test_set['essay_set'] == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each set will retain the original index, we want each of them to have their own indexing so that it is easier to match the essay and the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_1 = train_set_1.reset_index() # resets index\n",
    "train_set_2 = train_set_2.reset_index()\n",
    "train_set_3 = train_set_3.reset_index()\n",
    "train_set_4 = train_set_4.reset_index()\n",
    "train_set_5 = train_set_5.reset_index()\n",
    "train_set_6 = train_set_6.reset_index()\n",
    "train_set_7 = train_set_7.reset_index()\n",
    "train_set_8 = train_set_8.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1 = test_set_1.reset_index() # resets index\n",
    "test_set_2 = test_set_2.reset_index()\n",
    "test_set_3 = test_set_3.reset_index()\n",
    "test_set_4 = test_set_4.reset_index()\n",
    "test_set_5 = test_set_5.reset_index()\n",
    "test_set_6 = test_set_6.reset_index()\n",
    "test_set_7 = test_set_7.reset_index()\n",
    "test_set_8 = test_set_8.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use just the `essay` content and the respective `scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want for the whole dataset.\n",
    "# Commented out as we will work on individual datasets\n",
    "#essays = train_set['essay']\n",
    "#scores = train_set['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_1 = train_set_1['essay']\n",
    "scores_1 = train_set_1['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_2 = train_set_2['essay']\n",
    "scores_2 = train_set_2['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_3 = train_set_3['essay']\n",
    "scores_3 = train_set_3['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_4 = train_set_4['essay']\n",
    "scores_4 = train_set_4['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_5 = train_set_5['essay']\n",
    "scores_5 = train_set_5['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_6 = train_set_6['essay']\n",
    "scores_6 = train_set_6['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_7 = train_set_7['essay']\n",
    "scores_7 = train_set_7['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_8 = train_set_8['essay']\n",
    "scores_8 = train_set_8['domain1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the `domain1_score` column to `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1.columns = \"score\"\n",
    "scores_2.columns = \"score\"\n",
    "scores_3.columns = \"score\"\n",
    "scores_4.columns = \"score\"\n",
    "scores_5.columns = \"score\"\n",
    "scores_6.columns = \"score\"\n",
    "scores_7.columns = \"score\"\n",
    "scores_8.columns = \"score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE ABOVE NEEDS TO BE PUT INTO A LOOP BUT I LEFT IT AS IS BECAUSE YOU CAN PICK AND CHOOSE EASILY INSTEAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the essay sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take some time, be patient :-)\n",
    "e_set = EssaySet()\n",
    "\n",
    "for i in range(len(essays)):\n",
    "    e_set.add_essay(essays[i], scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = f_extractor.gen_length_feats(e_set)\n",
    "length_df = pd.DataFrame(\n",
    "    length, \n",
    "    columns = [\n",
    "        'chars', \n",
    "        'words', \n",
    "        'commas', \n",
    "        'apostrophes', \n",
    "        'punctuations', \n",
    "        'avg_word_length', \n",
    "        'POS', \n",
    "        'POS/total_words'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collate the essay prompts\n",
    "This consist of one essay from each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_prompts = []\n",
    "\n",
    "# Takes a bit of time also :)\n",
    "for i in range(1,9):\n",
    "    file = \"prompts/set\" + str(i) + \".txt\"\n",
    "    f = open(file, \"r\", encoding=\"latin-1\") # there are some 0x9x characters, hence need to specify encoding\n",
    "    essay_prompts.append(f.read())\n",
    "    \n",
    "def get_essay_prompt(essay_set):\n",
    "    return essay_prompts[essay_set-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure how this works\n",
    "e_set.update_prompt(get_essay_prompt(2))\n",
    "\n",
    "# Need more explanation on how this works - look into EASE\n",
    "\n",
    "prompts = f_extractor.gen_prompt_feats(e_set)\n",
    "prompts_df = pd.DataFrame(prompts, columns = ['prompt_words', 'prompt_words/total_words', 'synonym_words', 'synonym_words/total_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<essay_set.EssaySet at 0x2021d5dbf28>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another process that takes sometime to process\n",
    "unstemmed = util_functions.get_vocab_essays_count(e_set._text, e_set._score)\n",
    "stemmed = util_functions.get_vocab_essays_count(e_set._clean_stem_text, e_set._score)\n",
    "\n",
    "bow = list(map(lambda a,b:[a,b], unstemmed, stemmed))\n",
    "bow_df = pd.DataFrame(bow, columns = ['unstemmed', 'stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([length_df, prompts_df, bow_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.007590</td>\n",
       "      <td>524.330784</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.417457</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>584</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.672222</td>\n",
       "      <td>178.662900</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.524904</td>\n",
       "      <td>257.992218</td>\n",
       "      <td>0.988476</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>291</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2705.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.132827</td>\n",
       "      <td>521.653920</td>\n",
       "      <td>0.989856</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>547</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2394.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.778443</td>\n",
       "      <td>484.298031</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>591</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  2639.0  527.0    15.0         13.0          21.0         5.007590   \n",
       "1   841.0  180.0     5.0          2.0           3.0         4.672222   \n",
       "2  1181.0  261.0    12.0         15.0          14.0         4.524904   \n",
       "3  2705.0  527.0    22.0          6.0          31.0         5.132827   \n",
       "4  2394.0  501.0    25.0         15.0          34.0         4.778443   \n",
       "\n",
       "          POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "0  524.330784         0.994935         220.0                  0.417457   \n",
       "1  178.662900         0.992572          82.0                  0.455556   \n",
       "2  257.992218         0.988476         144.0                  0.551724   \n",
       "3  521.653920         0.989856         245.0                  0.464896   \n",
       "4  484.298031         0.966663         216.0                  0.431138   \n",
       "\n",
       "   synonym_words  synonym_words/total_words  unstemmed  stemmed  \n",
       "0          112.0                   0.212524        584      559  \n",
       "1           66.0                   0.366667        210      210  \n",
       "2           83.0                   0.318008        291      285  \n",
       "3          131.0                   0.248577        547      528  \n",
       "4          117.0                   0.233533        591      562  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export features to a file for next stage (optional)\n",
    "dataset = features.merge(scores, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.007590</td>\n",
       "      <td>524.330784</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.417457</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>584</td>\n",
       "      <td>559</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.672222</td>\n",
       "      <td>178.662900</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.524904</td>\n",
       "      <td>257.992218</td>\n",
       "      <td>0.988476</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>291</td>\n",
       "      <td>285</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2705.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.132827</td>\n",
       "      <td>521.653920</td>\n",
       "      <td>0.989856</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>547</td>\n",
       "      <td>528</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2394.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.778443</td>\n",
       "      <td>484.298031</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>591</td>\n",
       "      <td>562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  2639.0  527.0    15.0         13.0          21.0         5.007590   \n",
       "1   841.0  180.0     5.0          2.0           3.0         4.672222   \n",
       "2  1181.0  261.0    12.0         15.0          14.0         4.524904   \n",
       "3  2705.0  527.0    22.0          6.0          31.0         5.132827   \n",
       "4  2394.0  501.0    25.0         15.0          34.0         4.778443   \n",
       "\n",
       "          POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "0  524.330784         0.994935         220.0                  0.417457   \n",
       "1  178.662900         0.992572          82.0                  0.455556   \n",
       "2  257.992218         0.988476         144.0                  0.551724   \n",
       "3  521.653920         0.989856         245.0                  0.464896   \n",
       "4  484.298031         0.966663         216.0                  0.431138   \n",
       "\n",
       "   synonym_words  synonym_words/total_words  unstemmed  stemmed  domain1_score  \n",
       "0          112.0                   0.212524        584      559              4  \n",
       "1           66.0                   0.366667        210      210              1  \n",
       "2           83.0                   0.318008        291      285              2  \n",
       "3          131.0                   0.248577        547      528              4  \n",
       "4          117.0                   0.233533        591      562              4  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "       'avg_word_length', 'POS', 'POS/total_words', 'prompt_words',\n",
    "       'prompt_words/total_words', 'synonym_words',\n",
    "       'synonym_words/total_words', 'unstemmed', 'stemmed', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.007590</td>\n",
       "      <td>524.330784</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.417457</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>584</td>\n",
       "      <td>559</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.672222</td>\n",
       "      <td>178.662900</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.524904</td>\n",
       "      <td>257.992218</td>\n",
       "      <td>0.988476</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>291</td>\n",
       "      <td>285</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2705.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.132827</td>\n",
       "      <td>521.653920</td>\n",
       "      <td>0.989856</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>547</td>\n",
       "      <td>528</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2394.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.778443</td>\n",
       "      <td>484.298031</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>591</td>\n",
       "      <td>562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  2639.0  527.0    15.0         13.0          21.0         5.007590   \n",
       "1   841.0  180.0     5.0          2.0           3.0         4.672222   \n",
       "2  1181.0  261.0    12.0         15.0          14.0         4.524904   \n",
       "3  2705.0  527.0    22.0          6.0          31.0         5.132827   \n",
       "4  2394.0  501.0    25.0         15.0          34.0         4.778443   \n",
       "\n",
       "          POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "0  524.330784         0.994935         220.0                  0.417457   \n",
       "1  178.662900         0.992572          82.0                  0.455556   \n",
       "2  257.992218         0.988476         144.0                  0.551724   \n",
       "3  521.653920         0.989856         245.0                  0.464896   \n",
       "4  484.298031         0.966663         216.0                  0.431138   \n",
       "\n",
       "   synonym_words  synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0          112.0                   0.212524        584      559      4  \n",
       "1           66.0                   0.366667        210      210      1  \n",
       "2           83.0                   0.318008        291      285      2  \n",
       "3          131.0                   0.248577        547      528      4  \n",
       "4          117.0                   0.233533        591      562      4  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('maes_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can just use the features and score for the X and y but just to keep to certain convention if reading back from the CSV file above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:13].values.astype(float)\n",
    "y = dataset.iloc[:,14].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 1., 2., ..., 2., 3., 3.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 13)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53668756]\n",
      " [ 2.04630071]\n",
      " [-0.53668756]\n",
      " [ 0.75480657]\n",
      " [ 0.75480657]]\n"
     ]
    }
   ],
   "source": [
    "# To split the train / test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Have a look at the first few lines\n",
    "print(y_test[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "# maybe use poly and increase the degree\n",
    "regressor = SVR(kernel='rbf', gamma='auto', verbose=True)\n",
    "#regressor = SVR(kernel='poly', degree=5, gamma='auto', verbose=True)\n",
    "regressor.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test / Predict the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used yet as I don't have a sample X\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = sc_y.inverse_transform(y_pred).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Real Values  Predicted Values\n",
       "0          3.0               4.0\n",
       "1          5.0               4.0\n",
       "2          3.0               3.0\n",
       "3          4.0               3.0\n",
       "4          4.0               4.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Real Values':sc_y.inverse_transform(y_test.reshape(-1)),\n",
    "        'Predicted Values':y_pred\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = sc_y.inverse_transform(y_test).round()\n",
    "# y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.48081605360897567\n"
     ]
    }
   ],
   "source": [
    "# Need to wrap my head around this (where's the predictor)\n",
    "\n",
    "print(\"accuracy score:\", regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.6388888888888888\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score:\", accuracy_score(df['Real Values'], df['Predicted Values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584716505112203\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(sc_y.inverse_transform(y_test).round(), y_pred, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79591812, -0.79274203, -0.52192486, ..., -0.4291183 ,\n",
       "         1.3780107 , -0.99534758],\n",
       "       [-0.51618611, -0.55699135, -0.43044826, ..., -0.4291183 ,\n",
       "         0.36655217, -0.75801391],\n",
       "       [-0.63721302,  1.55901482, -1.25373772, ..., -2.02112129,\n",
       "        -6.02218295, -2.41310399],\n",
       "       ...,\n",
       "       [-0.34377985, -0.33274069,  0.57579442, ..., -0.04793449,\n",
       "         0.80414712, -0.37703091],\n",
       "       [ 1.40768504,  1.11626353,  0.85022424, ...,  0.64716541,\n",
       "        -0.97593924,  0.92830429],\n",
       "       [ 1.12452775,  1.25426394,  1.6735137 , ...,  1.20772984,\n",
       "        -0.18916275,  1.49665597]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1414,  289,    9, ...,   91,    0,  310],\n",
       "       [1659,  330,   10, ...,   91,    0,  348],\n",
       "       [1553,  698,    1, ...,   19,    0,   83],\n",
       "       ...,\n",
       "       [1810,  369,   21, ...,  108,    0,  409],\n",
       "       [3344,  621,   24, ...,  139,    0,  618],\n",
       "       [3096,  645,   33, ...,  164,    0,  709]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test = sc_X.inverse_transform(X_train)\n",
    "X_train_test = X_train_test.astype(int)\n",
    "X_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 3, 4, 4])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_test = sc_y.inverse_transform(y_train.reshape(-1))\n",
    "y_train_test = y_train_test.astype(int)\n",
    "y_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbclassifier = naive_bayes.MultinomialNB()\n",
    "nbclassifier.fit(X_train_test, y_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5038, 1022,   13, ...,  302,    0,  750],\n",
       "       [3220,  607,   29, ...,  132,    0,  704],\n",
       "       [1309,  271,    7, ...,   94,    0,  322],\n",
       "       ...,\n",
       "       [2254,  449,   19, ...,  108,    0,  515],\n",
       "       [2879,  572,   24, ...,  170,    0,  579],\n",
       "       [2494,  466,    8, ...,  119,    0,  572]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_test = sc_X.inverse_transform(X_test)\n",
    "X_test_test = X_test_test.astype(int)\n",
    "X_test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 2, 3, 5, 3, 3, 3, 4, 4, 4,\n",
       "       4, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 4, 1, 3, 4, 1, 2, 4, 3, 3, 4, 2,\n",
       "       3, 3, 3, 1, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 5, 3, 2, 3, 4,\n",
       "       3, 4, 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 3, 3, 2, 4, 3, 4, 3, 3, 4,\n",
       "       3, 4, 4, 4, 3, 4, 3, 4, 3, 1, 3, 3, 4, 4, 4, 4, 3, 2, 4, 2, 3, 2,\n",
       "       4, 2, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 3, 5, 2, 3, 3, 2, 5, 3, 3, 3,\n",
       "       3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 3, 5, 3, 4, 4, 4, 3, 3, 4, 2, 4, 4,\n",
       "       2, 4, 2, 4, 4, 3, 2, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 3, 4, 4, 4,\n",
       "       3, 3, 3, 1, 4, 5, 3, 4, 3, 4, 4, 3, 4, 4, 3, 1, 3, 4, 3, 4, 4, 4,\n",
       "       4, 4, 4, 3, 4, 4, 3, 4, 3, 4, 2, 4, 3, 5, 4, 3, 3, 4, 3, 4, 4, 3,\n",
       "       5, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 3, 3, 4, 3, 3, 3, 4, 6, 4, 3, 3,\n",
       "       3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 4, 2, 4, 4, 3, 2, 3, 3, 4, 4, 3, 2,\n",
       "       4, 4, 4, 3, 3, 4, 4, 5, 4, 4, 4, 4, 2, 4, 4, 4, 3, 4, 4, 5, 4, 3,\n",
       "       4, 3, 3, 4, 5, 4, 4, 3, 4, 2, 4, 3, 4, 3, 5, 4, 4, 3, 3, 4, 3, 4,\n",
       "       3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4,\n",
       "       3, 3, 3, 4, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 4, 2, 3, 4, 3, 3, 4, 3,\n",
       "       3, 3, 1, 3, 3, 4, 4, 4])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_test = sc_y.inverse_transform(y_test.reshape(-1))\n",
    "y_test_test = y_test_test.astype(int)\n",
    "y_test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3   3   0   1   0   0]\n",
      " [  5   6  10   1   0   1]\n",
      " [  0  15 104  37   1   3]\n",
      " [  0   3  61  81  11   0]\n",
      " [  0   0   1   4   4   4]\n",
      " [  0   0   0   0   1   0]]\n"
     ]
    }
   ],
   "source": [
    "y_predNB = nbclassifier.predict(X_test_test)\n",
    "\n",
    "cm = confusion_matrix(y_test_test, y_predNB)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.38      0.43      0.40         7\n",
      "           2       0.22      0.26      0.24        23\n",
      "           3       0.59      0.65      0.62       160\n",
      "           4       0.65      0.52      0.58       156\n",
      "           5       0.24      0.31      0.27        13\n",
      "           6       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       360\n",
      "   macro avg       0.35      0.36      0.35       360\n",
      "weighted avg       0.58      0.55      0.56       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rpt = classification_report(y_test_test, y_predNB)\n",
    "print(rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QWK Scores (Manual Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.04, 0.16, 0.36, 0.64, 1.  ],\n",
       "       [0.04, 0.  , 0.04, 0.16, 0.36, 0.64],\n",
       "       [0.16, 0.04, 0.  , 0.04, 0.16, 0.36],\n",
       "       [0.36, 0.16, 0.04, 0.  , 0.04, 0.16],\n",
       "       [0.64, 0.36, 0.16, 0.04, 0.  , 0.04],\n",
       "       [1.  , 0.64, 0.36, 0.16, 0.04, 0.  ]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(cm) # Just to get the same size as the confusion matrix from above\n",
    "w = np.zeros((N,N)) # create a matrix of N by N\n",
    "d = (N-1)**2 # the weighted portion\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(w)):\n",
    "        w[i][j] = float(((i-j)**2)/d) \n",
    "w # The weighted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_predNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_hist=np.zeros([N])\n",
    "for item in y_test_test: \n",
    "    act_hist[item-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hist=np.zeros([N])\n",
    "for item in y_predNB: \n",
    "    pred_hist[item-1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.6000e+01, 1.8900e+02, 1.2320e+03, 8.6800e+02, 1.1900e+02,\n",
       "        5.6000e+01],\n",
       "       [1.8400e+02, 6.2100e+02, 4.0480e+03, 2.8520e+03, 3.9100e+02,\n",
       "        1.8400e+02],\n",
       "       [1.2800e+03, 4.3200e+03, 2.8160e+04, 1.9840e+04, 2.7200e+03,\n",
       "        1.2800e+03],\n",
       "       [1.2480e+03, 4.2120e+03, 2.7456e+04, 1.9344e+04, 2.6520e+03,\n",
       "        1.2480e+03],\n",
       "       [1.0400e+02, 3.5100e+02, 2.2880e+03, 1.6120e+03, 2.2100e+02,\n",
       "        1.0400e+02],\n",
       "       [8.0000e+00, 2.7000e+01, 1.7600e+02, 1.2400e+02, 1.7000e+01,\n",
       "        8.0000e+00]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.outer(act_hist, pred_hist)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = E/E.sum()\n",
    "E.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = cm/cm.sum()\n",
    "cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.523820622785754"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=0\n",
    "den=0\n",
    "for i in range(len(w)):\n",
    "    for j in range(len(w)):\n",
    "        num+=w[i][j]*cm[i][j]\n",
    "        den+=w[i][j]*E[i][j]\n",
    "            \n",
    "weighted_kappa = (1 - (num/den))\n",
    "weighted_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWK scores output are from -1 to 1, where -1 means that it is totally wrong while 1 is a perfect match (classification).  The aim is to get as close as possible to 1, with a score of 0.6 being generally accepted as a good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QWK for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is a manual computation of the QWK, which we later found that it is already available as an option with the [Cohen Kappa Score](https://journals.sagepub.com/doi/10.1177/001316446002000104) in sklearn, when we set the weights to 'quadratic'.  Since it has already been manually coded above, we use the sklearn.metrics.cohen_kappa_score to validate our manual coded scoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 2, 3, 5, 3, 3, 3, 4, 4, 4,\n",
       "       4, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 4, 1, 3, 4, 1, 2, 4, 3, 3, 4, 2,\n",
       "       3, 3, 3, 1, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 5, 3, 2, 3, 4,\n",
       "       3, 4, 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 3, 3, 2, 4, 3, 4, 3, 3, 4,\n",
       "       3, 4, 4, 4, 3, 4, 3, 4, 3, 1, 3, 3, 4, 4, 4, 4, 3, 2, 4, 2, 3, 2,\n",
       "       4, 2, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 3, 5, 2, 3, 3, 2, 5, 3, 3, 3,\n",
       "       3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 3, 5, 3, 4, 4, 4, 3, 3, 4, 2, 4, 4,\n",
       "       2, 4, 2, 4, 4, 3, 2, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 3, 4, 4, 4,\n",
       "       3, 3, 3, 1, 4, 5, 3, 4, 3, 4, 4, 3, 4, 4, 3, 1, 3, 4, 3, 4, 4, 4,\n",
       "       4, 4, 4, 3, 4, 4, 3, 4, 3, 4, 2, 4, 3, 5, 4, 3, 3, 4, 3, 4, 4, 3,\n",
       "       5, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 3, 3, 4, 3, 3, 3, 4, 6, 4, 3, 3,\n",
       "       3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 4, 2, 4, 4, 3, 2, 3, 3, 4, 4, 3, 2,\n",
       "       4, 4, 4, 3, 3, 4, 4, 5, 4, 4, 4, 4, 2, 4, 4, 4, 3, 4, 4, 5, 4, 3,\n",
       "       4, 3, 3, 4, 5, 4, 4, 3, 4, 2, 4, 3, 4, 3, 5, 4, 4, 3, 3, 4, 3, 4,\n",
       "       3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4,\n",
       "       3, 3, 3, 4, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 4, 2, 3, 4, 3, 3, 4, 3,\n",
       "       3, 3, 1, 3, 3, 4, 4, 4])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 3, 3, 5, 3, 3, 3, 4, 3, 4, 4, 4, 3, 4, 4, 4, 3, 3, 3, 4, 4,\n",
       "       4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 3, 4, 4, 3, 4, 2, 3, 4, 3, 3, 2, 2,\n",
       "       3, 3, 3, 2, 4, 4, 3, 4, 4, 2, 4, 2, 3, 4, 3, 4, 3, 6, 3, 1, 4, 3,\n",
       "       4, 3, 5, 3, 3, 3, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 2, 3,\n",
       "       3, 4, 5, 4, 3, 4, 3, 4, 3, 1, 4, 2, 3, 3, 4, 3, 4, 6, 4, 3, 3, 4,\n",
       "       3, 1, 4, 3, 3, 3, 3, 5, 3, 4, 3, 3, 3, 4, 3, 3, 3, 1, 5, 3, 3, 3,\n",
       "       3, 4, 4, 4, 4, 5, 3, 6, 3, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 2, 4, 4,\n",
       "       2, 4, 2, 5, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3,\n",
       "       4, 3, 3, 1, 4, 6, 2, 3, 3, 3, 3, 3, 4, 3, 3, 1, 3, 3, 4, 4, 3, 4,\n",
       "       3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 1, 4, 3, 4, 3, 3, 4, 5, 3, 2, 4, 4,\n",
       "       5, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 5, 4, 2, 4,\n",
       "       2, 4, 3, 3, 3, 4, 4, 4, 2, 3, 4, 3, 3, 4, 4, 1, 3, 3, 4, 4, 3, 2,\n",
       "       5, 4, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 2, 3, 4, 5, 3, 4, 4, 6, 3, 3,\n",
       "       3, 2, 4, 4, 6, 4, 3, 3, 3, 3, 4, 3, 4, 4, 5, 4, 3, 6, 2, 4, 3, 3,\n",
       "       4, 3, 3, 5, 3, 4, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 4, 3, 3, 3, 5,\n",
       "       3, 3, 3, 3, 4, 3, 2, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
       "       4, 2, 2, 3, 2, 4, 4, 3])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28168493656854277\n",
      "0.5238206227857543\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_test_test, y_predNB))\n",
    "print(cohen_kappa_score(y_test_test, y_predNB, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the output of the QWK agreements, the score is just \"moderate agreement\".  Work now is to achieve substantial agreement.\n",
    "\n",
    "https://www.statisticshowto.com/cohens-kappa-statistic/\n",
    "\n",
    "In short, SVM works a little better than Naive Bayes for AES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
