{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook generates the features of all set 1 to set 8 and output them in csv format, using the modified version of feature_extractor.py\n",
    "\n",
    "### Output of the features of all sets are stored in the same directory as this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../ease')\n",
    "import create\n",
    "import grade \n",
    "import model_creator \n",
    "import predictor_extractor \n",
    "import predictor_set \n",
    "import util_functions\n",
    "import essay_set\n",
    "import feature_extractor\n",
    "\n",
    "from essay_set import EssaySet\n",
    "from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Essay Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_prompts = []\n",
    "\n",
    "for i in range(1,9):\n",
    "    file = \"../../prompts/set\" + str(i) + \".txt\"\n",
    "    f = open(file, \"r\", encoding=\"latin-1\") \n",
    "    essay_prompts.append(f.read())\n",
    "    \n",
    "def get_essay_prompt(essay_set):\n",
    "    return essay_prompts[essay_set-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 different essay sets.  As an overview:\n",
    "- Sets 1 & 2 are of persuasive/narrative in the form of letters\n",
    "- Sets 3, 4, 5 & 6 are source dependent response to a given essay\n",
    "- Sets 7 & 8 are of persuasive/narrative in the form of story writing essays\n",
    "\n",
    "These format makes it good for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"../../asap-aes/training_set_rel3.tsv\", sep='\\t', encoding=\"latin-1\")\n",
    "data_set['essay'] = [entry.lower() for entry in data_set['essay']] # lower case for all words in essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate csv file for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_csv(set_no):\n",
    "    # Filter dataset\n",
    "    data_set_n = data_set[data_set['essay_set'] == set_no]\n",
    "    data_set_n = data_set_n.reset_index()\n",
    "    \n",
    "    # Extract essay and its scores\n",
    "    essays = data_set_n['essay']\n",
    "    scores = data_set_n['domain1_score']\n",
    "    \n",
    "    # Create essay set\n",
    "    e_set = EssaySet()\n",
    "    for i in range(len(essays)):\n",
    "        e_set.add_essay(essays[i], scores[i])\n",
    "        \n",
    "    # Extract features\n",
    "    f_extractor = FeatureExtractor()\n",
    "    length = f_extractor.gen_length_feats(e_set)\n",
    "    length_df = pd.DataFrame(\n",
    "        length, \n",
    "        columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations', \n",
    "                   'avg_word_length', 'sentences', 'questions', 'avg_word_sentence',\n",
    "                   'POS', 'POS/total_words'])\n",
    "    \n",
    "    # Update Essay Set's prompt and extract prompt features\n",
    "    e_set.update_prompt(get_essay_prompt(set_no))\n",
    "    prompts = f_extractor.gen_prompt_feats(e_set)\n",
    "    prompts_df = pd.DataFrame(prompts, columns = [\n",
    "        'prompt_words', 'prompt_words/total_words', 'synonym_words', 'synonym_words/total_words'\n",
    "    ])\n",
    "    \n",
    "    # Get essays count for unstemmed and stemmed words\n",
    "    unstemmed = util_functions.get_vocab_essays_count(e_set._text, e_set._score)\n",
    "    stemmed = util_functions.get_vocab_essays_count(e_set._clean_stem_text, e_set._score)\n",
    "    bow = list(map(lambda a,b:[a,b], unstemmed, stemmed))\n",
    "    bow_df = pd.DataFrame(bow, columns = ['unstemmed', 'stemmed'])\n",
    "    \n",
    "    # Combine features of length, prompt and BoW (bag of words)\n",
    "    features = pd.concat([length_df, prompts_df, bow_df], axis=1, sort=False)\n",
    "    \n",
    "    # Merge with scores\n",
    "    dataset = features.merge(scores, left_index=True, right_index=True)\n",
    "    \n",
    "    # Export to dataset\n",
    "    dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "                       'avg_word_length', 'sentences', 'questions', 'avg_word_sentence',\n",
    "                       'POS', 'POS/total_words',\n",
    "                       'prompt_words', 'prompt_words/total_words', 'synonym_words',\n",
    "                       'synonym_words/total_words', 'unstemmed', 'stemmed',\n",
    "                       'score']\n",
    "    output_file = 'features_set' + str(set_no) + '.csv'\n",
    "    dataset.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all features csv files for set 1 to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_no in range(1, 9):\n",
    "    generate_features_csv(set_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
