{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains all the sample code on utilizing and extracting the 4 features as described in the NUS paper\n",
    "## This notebook is best read / understood side-by-side along with the python files in the ease directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fisher'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02d5e7b8ccdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcreate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrade\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_creator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredictor_extractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredictor_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\D_Drive\\AES-Monash\\asap-classification\\ease\\create.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Import modules that are dependent on the base path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_creator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpredictor_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\D_Drive\\AES-Monash\\asap-classification\\ease\\model_creator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# from itertools import chain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0messay_set\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEssaySet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\D_Drive\\AES-Monash\\asap-classification\\ease\\essay_set.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutil_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\D_Drive\\AES-Monash\\asap-classification\\ease\\util_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Requires aspell to be installed and added to the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfisher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0maspell_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"aspell\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fisher'"
     ]
    }
   ],
   "source": [
    "import create\n",
    "import grade \n",
    "import model_creator \n",
    "import predictor_extractor \n",
    "import predictor_set \n",
    "import util_functions\n",
    "import essay_set\n",
    "import feature_extractor\n",
    "\n",
    "from essay_set import EssaySet\n",
    "from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay1 = \"What do you want your child being exposed to?    In my own opinion I believe that @CAPS1 shouldn't be taken off the shelfs, because kids know what books are appropriate and innappropriate for them. This @CAPS2 why the library has @CAPS3 'kids' selction away from the 'adult' selction. They know their boundaries, and I don't think a librarian would let a child check out anything innappropriate. The library @CAPS2 a place for all sorts of books for anybody to come and have the right to check them out.   Also, @CAPS3 not just librarys', @CAPS1 can be seen or heard from schools, on the radio, t.v. and video games. Any where you go they're more likely going to see innapropriate actions that take place. So as a parent @CAPS3 your job to keep your child away from the things you don't want them to see or hear as much as possible.       The only way a child can be exposed to this @CAPS2 if their parents are unattended and irresponsible. Someday your kids will ask questions about that that kind of stuff they hear or see at school from other students. Just be open and honest at the right time but in a mature clean matter.      I know alot of young children these days who are being exposed a lot from violent video games, music about drugs, alchol, and sex. @CAPS1 really bothers me, because when I was their age I had no idea what @CAPS1 even was or meant. @CAPS1's wrong for the parents to let them be around that when they know @CAPS1 has a label on @CAPS1 saying '@CAPS10 @CAPS11', '@CAPS12'  or '@NUM1'.      Every time I hear a child say something about this aweseome video game or cool new song they heard on the radio about drugs, violence and etc, the children tend to get younger and younger every time. I don't understand why parents let them be exposed to that unecessary stuff\"\n",
    "essay2 = \"I've recently read a letter from Katherine Paterson about censorship in libraries. This is my opinion expressed in words about how I feel about the whole topic.     I've been reading ever since I hit first grade. To be honest, I couldn't tell you the names of @PERCENT1 of them because... well I just can't remember them. I'd like to think that it's because I have a horrible memory, but the truth is, they were just unappealing and unmemorable books. In all aspects, these books could be considered a waste of my life. Back then, I guess I only read them because I had nothing better to do, which is horrible because you should read books because you want to. I stopped reading books at around seventh grade because every book I picked up seemed to be extremely boring and it would take the first one hundred pages to actually have something decent happen.          Luckily, there is light at the end of this tunnel. Every now and then you come across a book that changes your life. Not literally, but it feels that way at the time (which is what you should feel). These are the books I remember. These 'life-changing' books all have things in them that would interest anyone who picks them up. Unfortunately, these things would be viewed as innappropriate to a select audience. For example, one book @MONTH1 talk about murder and be viewed as highly offensive by a person whose mother had been murdered, whereas the same book could be viewed as a piece of art by someone who has escaped the terribleness of death. Apparently it has gotten to the point where people would have the books censored or have them removed the libraries completely. I can stand idle no longer.     Books all go through a long, drawn-out process before they are shelf-ready. They need to be written, read, and selected to be published. For some books, this can take years. To completely alienate these books is extremely unfair to the time that was put into them. If you don't like the books, don't read them. It's as simple as that. If that's not enough reason for you, think of it this way: a book about a child getting raped and fighting to overcome it could serve as inspiration for rape victims. Or, a book about someone's parents dying could help a child in the same situation overcome the grief.     All books serve a purpose and should remain staunch on the shelves of local book stores, regardless of the methods used in them. Even if they don't appeal to you, it could be a completely different experience for someone else halfway across the world. Let these books serve their purpose.\"\n",
    "essay3 = \"Do you believe that certain materials, such as books, movies, magazines, etc ...,should be removed from the shelves if they are found offensive?   Most of the people made books that are inapropiate or offensive and sometimes are the life of the person or some experience that they have live. But the problem is that a lot of people are not realistic and they don't thing that those is the live of person's and can be the future of more people.Peple sometimes like to be the best and can make problems with other and that mekes them happy that is a sad thing because that's bad and also they can get of the point of do things so bad that they go to jail that's are some of the experiences that people shows  but others get it wrong. well thats their problem they can do what ever they whant .  I know that live is not easy but thank's of that material we can have a better live and also we can deffence ourselves.  Also i don't think that they  need to be remove that books or magazines because that's what help us to have e better life and also in the future, they need to be in there and if people found them offensive just don't go there or don't see that materials ignore them more people like those and thinkthat are helpful for them and for me.   I don't think that they goin to have a bad material in there, things that are inapropiate i thing that they can't have bad or  nasty information in a place that a lot of people go and not only adult also childre's  go too. And that's is more inappropriatefor kids. People know what's going on in there and some others persons know too. the world sometimes is a danger and sometimes help us for grows up that's is a good thing but we hace the choise if we want to read or have that information.  I remember one time when i read a book that is kind of sad but that help me to know better our life it was about two boys the book has a lot of bad words and talk really bad bad olso that explain more the book and we learn more about it. that's some things that help us with life. One of the books, magazines,music,movies .etc  that we can write is our life because we pass a lot of difficults times and that is a book of our life and can be a inappropriate book of our live.people like me give the tanks to those peoples that publish they experiences and at the same time help us\"\n",
    "text = [essay1, essay2, essay3]\n",
    "text = [entry.lower() for entry in text]\n",
    "text_score = [3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EssaySet()\n",
    "fe = FeatureExtractor()\n",
    "for i in range(len(text)):\n",
    "    e.add_essay(text[i], text_score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length: number of chars, words, commas, apostrophes, sentence (ending with punctuations) and average word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ian: Would need to create two more\n",
    "    - Average words per sentence\n",
    "    - Average sentences per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chars: [1787. 2616. 2362.]\n",
      " Words: [373. 527. 499.]\n",
      " Commas: [13. 20. 10.]\n",
      " Apostrophes: [19. 16. 18.]\n",
      " Punctuations: [17. 31. 18.]\n",
      " Average word length: [4.79088472 4.96394687 4.73346693]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description of the getting length section in feature_extractor.py\n",
    "\n",
    "The only function that deals with extracting the lengths is gen_length_feats()\n",
    "This returns all the lengths that we are looking for, so no further modificaction or work is needed\n",
    "\n",
    "However, the gen_length_feats returns the data (per set) as opposed to (per info)\n",
    "res = gen_length_feats(...)\n",
    "res[0] refers to the lengths of set 0\n",
    "res[1] refers to the lengths of set 1\n",
    "\n",
    "Hence, to obtain the data per info, we transpose them again. \n",
    "res[0] refers to number of chars where each index represents the essay \n",
    "res[1] refers to number of words\n",
    "res[2] refers to number of commas\n",
    "res[3] refers to number of apostrophes\n",
    "res[4] refers to number of sentences ending with punctuations (.?!)\n",
    "res[5] refers to number of chars per word (or average length of each word)\n",
    "\n",
    "The sample code to extract the lengths can be found below: \n",
    "\"\"\"\n",
    "\n",
    "res = fe.gen_length_feats(e).transpose()\n",
    "print(\" Chars: {}\\n Words: {}\\n Commas: {}\\n Apostrophes: {}\\n Punctuations: {}\\n Average word length: {}\"\n",
    "      .format(res[0], res[1], res[2], res[3], res[4], res[5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS: Number of bad POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " POS count: [367.98373984 520.98470363 490.98780488]\n",
      " POS count / total number of words: [0.98655158 0.98858578 0.9839435 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description of POS section in feature_extractor.py\n",
    "\n",
    "The number of bad POS can be obtained from the gen_length_feats function.\n",
    "\n",
    "res_pos[6] refers to the count of bad POS\n",
    "res_pos[7] refers to the cout of bad POS/ total number of words (per essay)\n",
    "\n",
    "A sample code of getting the count of bad POS is illustrated below. \n",
    "\"\"\"\n",
    "\n",
    "res_pos = fe.gen_length_feats(e).transpose()\n",
    "print(\" POS count: {}\\n POS count / total number of words: {}\"\n",
    "      .format(res_pos[6], res_pos[7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt: number of words appear in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of words in prompt: [144. 218. 252.]\n",
      " number of words in prompt / total words in essay: [0.38605898 0.41366224 0.50501002]\n",
      " number of words (a word or synonym) in prompt: [ 93. 134. 138.]\n",
      " number of words (a word or synonym) in prompt / total words in essay: [0.24932976 0.25426945 0.27655311]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description of prompt section in feature_extractor.py\n",
    "\n",
    "There is one function in the feature_extractor.py that directly gives us the prompts information. \n",
    "Specifically, we are looking for:\n",
    "\n",
    "1) number of words in prompt\n",
    "2) number of words in prompt/ total number of words in essay\n",
    "3) number of words which a word or synonym of a word in prompt\n",
    "4) number of words which a word or synonym of a word in prompt/ total number of words in essay\n",
    "\n",
    "However, feature_extractor uses the _prompt instance attribute in the EssaySet class, \n",
    "hence we need to update the prompt of the essay accordingly.\n",
    "\n",
    "The sample set of essays used above uses are extracted from set 2. \n",
    "We will update the EssaySet with the prompt of set 2. \n",
    "\n",
    "A sample code is illustrated below on updating the prompt of that set, \n",
    "followed by getting the prompts information.\n",
    "\"\"\"\n",
    "\n",
    "essay_prompts = []\n",
    "\n",
    "for i in range(1,9):\n",
    "    file = \"../prompts/set\" + str(i) + \".txt\"\n",
    "    f = open(file, \"r\")\n",
    "    essay_prompts.append(f.read())\n",
    "    \n",
    "def get_essay_prompt(essay_set):\n",
    "    return essay_prompts[essay_set-1]\n",
    "\n",
    "e.update_prompt(get_essay_prompt(2))\n",
    "\n",
    "res_prompt = fe.gen_prompt_feats(e).transpose()\n",
    "print(\" number of words in prompt: {}\\n number of words in prompt / total words in essay: {}\\n number of words (a word or synonym) in prompt: {}\\n number of words (a word or synonym) in prompt / total words in essay: {}\\n\"\n",
    "     .format(res_prompt[0], res_prompt[1], res_prompt[2], res_prompt[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW: Count of useful unigrams and bigrams (unstemmed, stemmed and spell corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of unstemmed words: [445, 621, 526]\n",
      " number of stemmed words: [435, 611, 507]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description of the bags of words section in feature_extractor.py:\n",
    "\n",
    "There are two main functions that deal with bag of words:\n",
    "1) gen_bag_feats: this generates BoW features from the input EssaySet and using a trained FeatureExtractor, \n",
    "                    but this returns an array of BoW features (only BoW)\n",
    "2) gen_feats: this generates BoW, length, and prompt features from the input EssaySet, \n",
    "                which also returns an array of features (BoW + length + prompt)\n",
    "\n",
    "The two functions above do not give us the cout of useful unigrams and bigrams of the EssaySet, \n",
    "so we look into the extraction of the unigrams and bigrams.\n",
    "\n",
    "The extraction of unigrams and bigrams can be found in the initialize_dictionaries function, \n",
    "which uses the get_vocab function in the util_functions.py module\n",
    "\n",
    "The get_vocab function in feature_extractor.py returns the list of ngrams words of all essays in the essay set. \n",
    "This means that we cannot determine the essay where the unigram/bigram belongs to. \n",
    "\n",
    "An helper function get_vocab_essays_count is added in util_functions.py \n",
    "This returns the count of unigrams/bigrams of each essay in the set.\n",
    "## NOTE: THIS FUNCTION REQUIRES FURTHER INSPECTION OF ITS CORRECTNESS, KINDLY CHECK\n",
    "\n",
    "The sample code to extract unigrams and bigrams unstemmed and stemmed can be found below:\n",
    "\"\"\"\n",
    "\n",
    "unstemmed = util_functions.get_vocab_essays_count(e._text, e._score)\n",
    "stemmed = util_functions.get_vocab_essays_count(e._clean_stem_text, e._score)\n",
    "print(\" number of unstemmed words: {}\\n number of stemmed words: {}\\n\"\n",
    "     .format(unstemmed, stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
