{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M - Automated Essay Scoring\n",
    "_School of Information Technology_<br>\n",
    "_Monash University Malaysia_<br>\n",
    "(c) Copyright 2020, Ian Tan & Jun Qing Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "- Import libraries\n",
    "- Read dataset (ASAP)\n",
    "- Extract features (into file) using EASE\n",
    "- Conduct machine learning (Sci-kit Learn libraries)\n",
    "- Evaluate (QWK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm #SVR is in SVM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the EASE functions, which is located in the ease folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'ease')\n",
    "import create\n",
    "import grade \n",
    "import model_creator \n",
    "import predictor_extractor \n",
    "import predictor_set \n",
    "import util_functions\n",
    "import essay_set\n",
    "import feature_extractor\n",
    "\n",
    "from essay_set import EssaySet\n",
    "from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AES (Hewlett Foundation dataset from Kaggle) in the folder \"aes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"asap-aes/training_set_rel3.tsv\", sep='\\t', encoding=\"latin-1\")\n",
    "#train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Am filtering just for this current exercise.\n",
    "# Set 2 has 1,800 essays, sufficient for current work\n",
    "train_set = train_set[train_set['essay_set'] == 2]  # filter for set 2\n",
    "#train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.reset_index() # resets index\n",
    "#train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['essay'] = [entry.lower() for entry in train_set['essay']] # lower case for all words in essay\n",
    "#train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = train_set['essay']\n",
    "scores = train_set['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.columns = \"score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the essay sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take some time, be patient :-)\n",
    "e_set = EssaySet()\n",
    "\n",
    "for i in range(len(essays)):\n",
    "    e_set.add_essay(essays[i], scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = f_extractor.gen_length_feats(e_set)\n",
    "length_df = pd.DataFrame(\n",
    "    length, \n",
    "    columns = [\n",
    "        'chars', \n",
    "        'words', \n",
    "        'commas', \n",
    "        'apostrophes', \n",
    "        'punctuations', \n",
    "        'avg_word_length', \n",
    "        'POS', \n",
    "        'POS/total_words'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collate the essay prompts\n",
    "This consist of one essay from each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_prompts = []\n",
    "\n",
    "# Takes a bit of time also :)\n",
    "for i in range(1,9):\n",
    "    file = \"prompts/set\" + str(i) + \".txt\"\n",
    "    f = open(file, \"r\", encoding=\"latin-1\") # there are some 0x9x characters, hence need to specify encoding\n",
    "    essay_prompts.append(f.read())\n",
    "    \n",
    "def get_essay_prompt(essay_set):\n",
    "    return essay_prompts[essay_set-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure how this works\n",
    "e_set.update_prompt(get_essay_prompt(2))\n",
    "\n",
    "# Need more explanation on how this works - look into EASE\n",
    "\n",
    "prompts = f_extractor.gen_prompt_feats(e_set)\n",
    "prompts_df = pd.DataFrame(prompts, columns = ['prompt_words', 'prompt_words/total_words', 'synonym_words', 'synonym_words/total_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<essay_set.EssaySet at 0x24020448860>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another process that takes sometime to process\n",
    "unstemmed = util_functions.get_vocab_essays_count(e_set._text, e_set._score)\n",
    "stemmed = util_functions.get_vocab_essays_count(e_set._clean_stem_text, e_set._score)\n",
    "\n",
    "bow = list(map(lambda a,b:[a,b], unstemmed, stemmed))\n",
    "bow_df = pd.DataFrame(bow, columns = ['unstemmed', 'stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([length_df, prompts_df, bow_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.007590</td>\n",
       "      <td>524.330784</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.417457</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>584</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.672222</td>\n",
       "      <td>178.662900</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.524904</td>\n",
       "      <td>257.992218</td>\n",
       "      <td>0.988476</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>291</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2705.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.132827</td>\n",
       "      <td>521.653920</td>\n",
       "      <td>0.989856</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>547</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2394.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.778443</td>\n",
       "      <td>484.298031</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>591</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  2639.0  527.0    15.0         13.0          21.0         5.007590   \n",
       "1   841.0  180.0     5.0          2.0           3.0         4.672222   \n",
       "2  1181.0  261.0    12.0         15.0          14.0         4.524904   \n",
       "3  2705.0  527.0    22.0          6.0          31.0         5.132827   \n",
       "4  2394.0  501.0    25.0         15.0          34.0         4.778443   \n",
       "\n",
       "          POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "0  524.330784         0.994935         220.0                  0.417457   \n",
       "1  178.662900         0.992572          82.0                  0.455556   \n",
       "2  257.992218         0.988476         144.0                  0.551724   \n",
       "3  521.653920         0.989856         245.0                  0.464896   \n",
       "4  484.298031         0.966663         216.0                  0.431138   \n",
       "\n",
       "   synonym_words  synonym_words/total_words  unstemmed  stemmed  \n",
       "0          112.0                   0.212524        584      559  \n",
       "1           66.0                   0.366667        210      210  \n",
       "2           83.0                   0.318008        291      285  \n",
       "3          131.0                   0.248577        547      528  \n",
       "4          117.0                   0.233533        591      562  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export features to a file for next stage (optional)\n",
    "dataset = features.merge(scores, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.007590</td>\n",
       "      <td>524.330784</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.417457</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>584</td>\n",
       "      <td>559</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.672222</td>\n",
       "      <td>178.662900</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.524904</td>\n",
       "      <td>257.992218</td>\n",
       "      <td>0.988476</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>291</td>\n",
       "      <td>285</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2705.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.132827</td>\n",
       "      <td>521.653920</td>\n",
       "      <td>0.989856</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>547</td>\n",
       "      <td>528</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2394.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.778443</td>\n",
       "      <td>484.298031</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>591</td>\n",
       "      <td>562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  2639.0  527.0    15.0         13.0          21.0         5.007590   \n",
       "1   841.0  180.0     5.0          2.0           3.0         4.672222   \n",
       "2  1181.0  261.0    12.0         15.0          14.0         4.524904   \n",
       "3  2705.0  527.0    22.0          6.0          31.0         5.132827   \n",
       "4  2394.0  501.0    25.0         15.0          34.0         4.778443   \n",
       "\n",
       "          POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "0  524.330784         0.994935         220.0                  0.417457   \n",
       "1  178.662900         0.992572          82.0                  0.455556   \n",
       "2  257.992218         0.988476         144.0                  0.551724   \n",
       "3  521.653920         0.989856         245.0                  0.464896   \n",
       "4  484.298031         0.966663         216.0                  0.431138   \n",
       "\n",
       "   synonym_words  synonym_words/total_words  unstemmed  stemmed  domain1_score  \n",
       "0          112.0                   0.212524        584      559              4  \n",
       "1           66.0                   0.366667        210      210              1  \n",
       "2           83.0                   0.318008        291      285              2  \n",
       "3          131.0                   0.248577        547      528              4  \n",
       "4          117.0                   0.233533        591      562              4  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['chars', 'words', 'commas', 'apostrophes', 'punctuations',\n",
    "       'avg_word_length', 'POS', 'POS/total_words', 'prompt_words',\n",
    "       'prompt_words/total_words', 'synonym_words',\n",
    "       'synonym_words/total_words', 'unstemmed', 'stemmed', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2639.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.007590</td>\n",
       "      <td>524.330784</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.417457</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>584</td>\n",
       "      <td>559</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.672222</td>\n",
       "      <td>178.662900</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.524904</td>\n",
       "      <td>257.992218</td>\n",
       "      <td>0.988476</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>291</td>\n",
       "      <td>285</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2705.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.132827</td>\n",
       "      <td>521.653920</td>\n",
       "      <td>0.989856</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>547</td>\n",
       "      <td>528</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2394.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.778443</td>\n",
       "      <td>484.298031</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.431138</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>591</td>\n",
       "      <td>562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0  2639.0  527.0    15.0         13.0          21.0         5.007590   \n",
       "1   841.0  180.0     5.0          2.0           3.0         4.672222   \n",
       "2  1181.0  261.0    12.0         15.0          14.0         4.524904   \n",
       "3  2705.0  527.0    22.0          6.0          31.0         5.132827   \n",
       "4  2394.0  501.0    25.0         15.0          34.0         4.778443   \n",
       "\n",
       "          POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "0  524.330784         0.994935         220.0                  0.417457   \n",
       "1  178.662900         0.992572          82.0                  0.455556   \n",
       "2  257.992218         0.988476         144.0                  0.551724   \n",
       "3  521.653920         0.989856         245.0                  0.464896   \n",
       "4  484.298031         0.966663         216.0                  0.431138   \n",
       "\n",
       "   synonym_words  synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0          112.0                   0.212524        584      559      4  \n",
       "1           66.0                   0.366667        210      210      1  \n",
       "2           83.0                   0.318008        291      285      2  \n",
       "3          131.0                   0.248577        547      528      4  \n",
       "4          117.0                   0.233533        591      562      4  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('maes_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can just use the features and score for the X and y but just to keep to certain convention if reading back from the CSV file above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:13].values.astype(float)\n",
    "y = dataset.iloc[:,14].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 1., 2., ..., 2., 3., 3.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 13)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53668756]\n",
      " [ 2.04630071]\n",
      " [-0.53668756]\n",
      " [ 0.75480657]\n",
      " [ 0.75480657]]\n"
     ]
    }
   ],
   "source": [
    "# To split the train / test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Have a look at the first few lines\n",
    "print(y_test[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "# maybe use poly and increase the degree\n",
    "regressor = SVR(kernel='rbf', gamma='auto', verbose=True)\n",
    "regressor.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test / Predict the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used yet as I don't have a sample X\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = sc_y.inverse_transform(y_pred).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Real Values  Predicted Values\n",
       "0          3.0               4.0\n",
       "1          5.0               4.0\n",
       "2          3.0               3.0\n",
       "3          4.0               3.0\n",
       "4          4.0               4.0\n",
       "5          4.0               3.0\n",
       "6          4.0               4.0\n",
       "7          3.0               3.0\n",
       "8          4.0               4.0\n",
       "9          3.0               3.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Real Values':sc_y.inverse_transform(y_test.reshape(-1)),\n",
    "        'Predicted Values':y_pred\n",
    "    }\n",
    ")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 3., 3., 4., 3., 4., 3., 4., 3., 4., 4., 3., 3., 3., 4., 3.,\n",
       "       3., 3., 3., 4., 4., 3., 3., 3., 3., 3., 3., 4., 3., 4., 4., 3., 4.,\n",
       "       3., 4., 4., 1., 3., 4., 3., 4., 4., 2., 3., 3., 3., 2., 4., 4., 3.,\n",
       "       4., 4., 3., 3., 2., 3., 4., 3., 4., 3., 5., 3., 2., 4., 3., 3., 3.,\n",
       "       4., 3., 3., 4., 3., 3., 4., 3., 4., 4., 4., 3., 4., 2., 4., 3., 4.,\n",
       "       3., 3., 4., 3., 4., 4., 3., 3., 3., 4., 4., 3., 2., 3., 3., 3., 3.,\n",
       "       4., 3., 3., 4., 4., 3., 3., 3., 4., 3., 4., 3., 3., 4., 4., 4., 4.,\n",
       "       4., 3., 3., 3., 4., 3., 2., 3., 2., 4., 3., 3., 3., 4., 4., 3., 4.,\n",
       "       4., 4., 3., 4., 3., 3., 3., 4., 3., 3., 4., 4., 4., 3., 4., 3., 4.,\n",
       "       4., 3., 4., 3., 4., 4., 3., 3., 3., 3., 3., 3., 4., 3., 4., 3., 4.,\n",
       "       4., 3., 3., 4., 4., 3., 3., 4., 3., 2., 3., 4., 2., 3., 3., 3., 4.,\n",
       "       3., 4., 3., 4., 2., 3., 3., 4., 4., 3., 4., 3., 4., 4., 4., 4., 3.,\n",
       "       4., 4., 3., 4., 2., 4., 3., 4., 3., 3., 3., 4., 3., 3., 4., 4., 4.,\n",
       "       4., 4., 3., 4., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4., 3., 3., 4.,\n",
       "       4., 4., 3., 3., 3., 3., 3., 3., 3., 3., 4., 3., 4., 3., 4., 3., 3.,\n",
       "       4., 4., 2., 3., 4., 4., 4., 3., 3., 4., 4., 3., 4., 3., 3., 4., 4.,\n",
       "       3., 3., 4., 4., 2., 3., 4., 4., 3., 4., 4., 5., 3., 3., 3., 3., 4.,\n",
       "       4., 4., 4., 4., 3., 3., 3., 4., 3., 4., 4., 4., 3., 4., 4., 3., 4.,\n",
       "       3., 4., 4., 3., 4., 4., 3., 4., 3., 3., 3., 3., 4., 4., 3., 3., 4.,\n",
       "       3., 3., 4., 3., 3., 3., 4., 3., 3., 3., 3., 4., 4., 3., 3., 4., 3.,\n",
       "       3., 4., 4., 3., 3., 2., 3., 4., 3., 3., 3., 3., 4., 3., 2., 3., 3.,\n",
       "       4., 4., 4.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 5., 3., 4., 4., 4., 4., 3., 4., 3., 4., 3., 4., 2., 3., 5., 3.,\n",
       "       3., 3., 4., 4., 4., 4., 4., 3., 3., 3., 4., 3., 3., 3., 4., 3., 4.,\n",
       "       1., 3., 4., 1., 2., 4., 3., 3., 4., 2., 3., 3., 3., 1., 4., 4., 4.,\n",
       "       4., 4., 3., 3., 3., 4., 3., 3., 4., 3., 5., 3., 2., 3., 4., 3., 4.,\n",
       "       4., 3., 4., 3., 4., 4., 3., 3., 4., 4., 4., 3., 3., 2., 4., 3., 4.,\n",
       "       3., 3., 4., 3., 4., 4., 4., 3., 4., 3., 4., 3., 1., 3., 3., 4., 4.,\n",
       "       4., 4., 3., 2., 4., 2., 3., 2., 4., 2., 4., 3., 3., 4., 4., 3., 4.,\n",
       "       4., 3., 3., 3., 5., 2., 3., 3., 2., 5., 3., 3., 3., 3., 4., 3., 3.,\n",
       "       4., 4., 3., 3., 3., 3., 3., 5., 3., 4., 4., 4., 3., 3., 4., 2., 4.,\n",
       "       4., 2., 4., 2., 4., 4., 3., 2., 3., 3., 3., 4., 4., 3., 4., 4., 3.,\n",
       "       4., 3., 3., 4., 4., 4., 3., 3., 3., 1., 4., 5., 3., 4., 3., 4., 4.,\n",
       "       3., 4., 4., 3., 1., 3., 4., 3., 4., 4., 4., 4., 4., 4., 3., 4., 4.,\n",
       "       3., 4., 3., 4., 2., 4., 3., 5., 4., 3., 3., 4., 3., 4., 4., 3., 5.,\n",
       "       3., 4., 4., 4., 4., 3., 4., 3., 2., 3., 3., 3., 4., 3., 3., 3., 4.,\n",
       "       6., 4., 3., 3., 3., 4., 3., 3., 3., 4., 4., 3., 3., 3., 4., 2., 4.,\n",
       "       4., 3., 2., 3., 3., 4., 4., 3., 2., 4., 4., 4., 3., 3., 4., 4., 5.,\n",
       "       4., 4., 4., 4., 2., 4., 4., 4., 3., 4., 4., 5., 4., 3., 4., 3., 3.,\n",
       "       4., 5., 4., 4., 3., 4., 2., 4., 3., 4., 3., 5., 4., 4., 3., 3., 4.,\n",
       "       3., 4., 3., 3., 4., 4., 4., 3., 3., 3., 3., 3., 4., 4., 4., 3., 3.,\n",
       "       4., 3., 4., 3., 3., 3., 4., 3., 3., 3., 4., 4., 3., 3., 3., 4., 3.,\n",
       "       3., 3., 4., 3., 4., 2., 3., 4., 3., 3., 4., 3., 3., 3., 1., 3., 3.,\n",
       "       4., 4., 4.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = sc_y.inverse_transform(y_test).round()\n",
    "y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: -31.814631081200737\n"
     ]
    }
   ],
   "source": [
    "# Need to wrap my head around this (where's the predictor)\n",
    "# https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226\n",
    "print(\"accuracy score:\", regressor.score(X_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.6388888888888888\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score:\", accuracy_score(df['Real Values'], df['Predicted Values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
